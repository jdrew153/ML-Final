
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Preprocessing Data - Renaming Training Image Files with Unique Numbering &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'main';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to your Jupyter Book" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Preprocessing Data -  Renaming Training Image Files with Unique Numbering</a></li>










</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fmain.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/main.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Preprocessing Data -  Renaming Training Image Files with Unique Numbering</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Preprocessing Data -  Renaming Training Image Files with Unique Numbering</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#functionality-overview">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-details">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#count-files-directory-function"><strong><code class="docutils literal notranslate"><span class="pre">count_files(directory)</span></code> Function</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-image-count"><strong>Training Image Count</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#file-renaming-logic"><strong>File Renaming Logic</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-step-for-pytorch-cnn-creating-json-annotations-for-dataloader-pytorch">Preprocessing Step for Pytorch CNN: Creating JSON Annotations for DataLoader (Pytorch)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-json-labels-directory-output-file-function"><strong><code class="docutils literal notranslate"><span class="pre">create_json_labels(directory,</span> <span class="pre">output_file)</span></code> Function</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-preparation-custom-dataset-and-dataloader-for-bottle-classification">PyTorch Preparation: Custom Dataset and DataLoader for Bottle Classification</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custombottledataset-class"><strong><code class="docutils literal notranslate"><span class="pre">CustomBottleDataset</span></code> Class</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initialization-init"><strong>Initialization (<code class="docutils literal notranslate"><span class="pre">__init__</span></code>)</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#length-len"><strong>Length (<code class="docutils literal notranslate"><span class="pre">__len__</span></code>)</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#item-retrieval-getitem"><strong>Item Retrieval (<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>)</strong>:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformations-t"><strong>Transformations (<code class="docutils literal notranslate"><span class="pre">T</span></code>)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-dataloader"><strong>Dataset and DataLoader</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-cnn-using-reference-model-defect-identification-neural-network">Custom CNN Using Reference Model: Defect Identification Neural Network</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defectidentificationnetwork-class"><strong><code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> Class</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7"><strong>Initialization (<code class="docutils literal notranslate"><span class="pre">__init__</span></code>)</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-forward"><strong>Forward Pass (<code class="docutils literal notranslate"><span class="pre">forward</span></code>)</strong>:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#training-step-training-and-tracking-the-defect-identification-model">Training Step: Training and Tracking the Defect Identification Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-initialization-and-loading"><strong>1. Model Initialization and Loading</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-configuration"><strong>2. Training Configuration</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop"><strong>3. Training Loop</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-tracking-and-reporting"><strong>4. Loss Tracking and Reporting</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-step-model-evaluation-and-performance-reporting">Validation Step: Model Evaluation and Performance Reporting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-dataloader-setup"><strong>1. Dataset and DataLoader Setup</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-initialization"><strong>2. Model Initialization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-loop"><strong>3. Evaluation Loop</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-calculation"><strong>4. Metrics Calculation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-files"><strong>5. Output Files</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-for-model-using-sharpened-images-sharpening-images-and-generating-annotations">Preprocessing for Model Using Sharpened Images: Sharpening Images and Generating Annotations</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sharpening-images"><strong>1. Sharpening Images</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-parameters"><strong>Input Parameters</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14"><strong>Steps</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output"><strong>Output</strong>:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-sharpened-image-annotations"><strong>2. Generating Sharpened Image Annotations</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#directory-structure"><strong>3. Directory Structure</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#training-model-for-sharpened-images-training-the-model-on-sharpened-images">Training Model for Sharpened Images: Training the Model on Sharpened Images</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences">Key Differences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-summary">Workflow Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-preparation">1. Dataset Preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-setup">2. Model Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3. Training Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-tracking">4. Loss Tracking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization">5. Visualization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outputs">Outputs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-weights">1. Model Weights:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-tracker-file">2. Loss Tracker File:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-step-for-model-using-sharpened-images-evaluation-on-sharpened-test-images">Validation Step for Model Using Sharpened Images: Evaluation on Sharpened Test Images</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Key Differences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Outputs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-and-misclassification-rate">1. Accuracy and Misclassification Rate:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-the-base-model-visualizing-loss-over-epochs">Analyzing the Base Model: Visualizing Loss Over Epochs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-steps">Key Steps</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-for-csv-file"><strong>1. Check for CSV File</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#read-loss-data"><strong>2. Read Loss Data</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#code-summary-visualizing-loss-over-epochs-for-sharpened-images">Code Summary: Visualizing Loss Over Epochs for Sharpened Images</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Key Steps</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23"><strong>1. Check for CSV File</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#read-and-process-data"><strong>2. Read and Process Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss"><strong>3. Plot the Loss</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="preprocessing-data-renaming-training-image-files-with-unique-numbering">
<h1>Preprocessing Data -  Renaming Training Image Files with Unique Numbering<a class="headerlink" href="#preprocessing-data-renaming-training-image-files-with-unique-numbering" title="Link to this heading">#</a></h1>
<section id="functionality-overview">
<h2>Functionality Overview<a class="headerlink" href="#functionality-overview" title="Link to this heading">#</a></h2>
<p>This script performs two main tasks related to organizing and renaming image files in a training dataset obtained from The MVTec anomaly detection dataset (MVTec AD) <a class="reference external" href="https://www.mvtec.com/company/research/datasets/mvtec-ad:">https://www.mvtec.com/company/research/datasets/mvtec-ad:</a></p>
<ol class="arabic simple">
<li><p><strong>Counting Files in Training Image Directories</strong>:</p>
<ul class="simple">
<li><p>The script iterates through a set of directories that categorize training images (e.g., <code class="docutils literal notranslate"><span class="pre">broken_large</span></code>, <code class="docutils literal notranslate"><span class="pre">broken_small</span></code>, etc.).</p></li>
<li><p>It counts the total number of files in each directory using the <code class="docutils literal notranslate"><span class="pre">count_files</span></code> function.</p></li>
<li><p>The total count of files across all training directories is displayed.</p></li>
</ul>
</li>
<li><p><strong>Renaming Image Files for Unique Identification</strong>:</p>
<ul class="simple">
<li><p>The script processes image files in a separate <code class="docutils literal notranslate"><span class="pre">bottle/test</span></code> directory for each category.</p></li>
<li><p>Files are renamed with a unique identifier in the format: <code class="docutils literal notranslate"><span class="pre">&lt;category_name&gt;_&lt;index&gt;.jpg</span></code>.</p></li>
<li><p>For example, the file <code class="docutils literal notranslate"><span class="pre">image1.jpg</span></code> in the <code class="docutils literal notranslate"><span class="pre">broken_large</span></code> category would be renamed to <code class="docutils literal notranslate"><span class="pre">broken_large_0.jpg</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="code-details">
<h2>Code Details<a class="headerlink" href="#code-details" title="Link to this heading">#</a></h2>
<section id="count-files-directory-function">
<h3><strong><code class="docutils literal notranslate"><span class="pre">count_files(directory)</span></code> Function</strong><a class="headerlink" href="#count-files-directory-function" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Iterates through all files in a given directory (including subdirectories).</p></li>
<li><p>Returns the total count of files.</p></li>
</ul>
</section>
<section id="training-image-count">
<h3><strong>Training Image Count</strong><a class="headerlink" href="#training-image-count" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Uses the <code class="docutils literal notranslate"><span class="pre">count_files</span></code> function to count the number of training images in each of the following directories:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">broken_large</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">broken_small</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">contamination</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">good</span></code></p></li>
</ul>
</li>
<li><p>Prints the file count for each category and the total count of training images.</p></li>
</ul>
</section>
<section id="file-renaming-logic">
<h3><strong>File Renaming Logic</strong><a class="headerlink" href="#file-renaming-logic" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Iterates through the directories in the <code class="docutils literal notranslate"><span class="pre">bottle/test</span></code> path.</p></li>
<li><p>Renames each file in the directory with a unique name based on its category and index (e.g., <code class="docutils literal notranslate"><span class="pre">broken_large_0.jpg</span></code>).</p></li>
<li><p>The files are renamed to correspond to their specific class, <code class="docutils literal notranslate"><span class="pre">good,</span> <span class="pre">broken</span> <span class="pre">small,</span> <span class="pre">broken</span> <span class="pre">large,</span> <span class="pre">contamination,</span> <span class="pre">good</span></code>,</p></li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## trying to rename the training image files to have unique numbering</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">count_files</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
    <span class="n">file_ct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
        <span class="n">file_ct</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">files</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">file_ct</span>

<span class="n">training_img_dirs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;broken_large&#39;</span><span class="p">,</span> <span class="s1">&#39;broken_small&#39;</span><span class="p">,</span> <span class="s1">&#39;contamination&#39;</span><span class="p">,</span> <span class="s1">&#39;good&#39;</span><span class="p">]</span>

<span class="n">total_train_file_count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">img_dir</span> <span class="ow">in</span> <span class="n">training_img_dirs</span><span class="p">:</span>
    <span class="n">full_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;bottle/train&#39;</span><span class="p">,</span> <span class="n">img_dir</span><span class="p">)</span>
    <span class="n">file_ct</span> <span class="o">=</span> <span class="n">count_files</span><span class="p">(</span><span class="n">full_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">img_dir</span><span class="si">}</span><span class="s1"> has </span><span class="si">{</span><span class="n">file_ct</span><span class="si">}</span><span class="s1"> files&#39;</span><span class="p">)</span>

    <span class="n">total_train_file_count</span> <span class="o">+=</span> <span class="n">file_ct</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Total training files: </span><span class="si">{</span><span class="n">total_train_file_count</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_img_dirs</span><span class="p">)):</span>
    <span class="n">img_dir</span> <span class="o">=</span> <span class="n">training_img_dirs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">full_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;bottle/test&#39;</span><span class="p">,</span> <span class="n">img_dir</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">file</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">full_path</span><span class="p">)):</span>
        <span class="n">new_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">img_dir</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s1">.jpg&#39;</span>
        <span class="n">os</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">full_path</span><span class="p">,</span> <span class="n">file</span><span class="p">),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">full_path</span><span class="p">,</span> <span class="n">new_name</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="n">new_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    
  
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>broken_large has 0 files
broken_small has 0 files
contamination has 0 files
good has 0 files
Total training files: 0
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">27</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span> <span class="n">img_dir</span> <span class="o">=</span> <span class="n">training_img_dirs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span> <span class="n">full_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;bottle/test&#39;</span><span class="p">,</span> <span class="n">img_dir</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">27</span> <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">file</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">full_path</span><span class="p">)):</span>
<span class="g g-Whitespace">     </span><span class="mi">28</span>     <span class="n">new_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">img_dir</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s1">.jpg&#39;</span>
<span class="g g-Whitespace">     </span><span class="mi">29</span>     <span class="n">os</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">full_path</span><span class="p">,</span> <span class="n">file</span><span class="p">),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">full_path</span><span class="p">,</span> <span class="n">new_name</span><span class="p">))</span>

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;bottle/test/broken_large&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="preprocessing-step-for-pytorch-cnn-creating-json-annotations-for-dataloader-pytorch">
<h1>Preprocessing Step for Pytorch CNN: Creating JSON Annotations for DataLoader (Pytorch)<a class="headerlink" href="#preprocessing-step-for-pytorch-cnn-creating-json-annotations-for-dataloader-pytorch" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>Functionality Overview<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>This code block generates JSON annotation files to map image filenames to their respective labels to use in the DataLoader Class provided by PyTorch. The annotations are created for both the training and testing datasets because training and testing the model requires separate DataLoaders for the respective datasets.</p>
<p>The images are saved containing the class name <code class="docutils literal notranslate"><span class="pre">i.e</span> <span class="pre">broken</span> <span class="pre">small</span></code> so that the class name for the file can easily be identified.</p>
</section>
<hr class="docutils" />
<section id="id2">
<h2>Code Details<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<section id="create-json-labels-directory-output-file-function">
<h3><strong><code class="docutils literal notranslate"><span class="pre">create_json_labels(directory,</span> <span class="pre">output_file)</span></code> Function</strong><a class="headerlink" href="#create-json-labels-directory-output-file-function" title="Link to this heading">#</a></h3>
<p>This function is responsible for generating JSON files containing image-to-label mappings.</p>
<section id="steps">
<h4>Steps:<a class="headerlink" href="#steps" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Remove Existing Annotation File</strong>:</p>
<ul class="simple">
<li><p>If the specified output JSON file already exists, it is deleted to ensure a fresh start.</p></li>
</ul>
</li>
<li><p><strong>Initialize Label Mapping</strong>:</p>
<ul class="simple">
<li><p>A dictionary, <code class="docutils literal notranslate"><span class="pre">label_img_mapping</span></code>, is created to store the mapping between filenames and their corresponding labels.</p></li>
</ul>
</li>
<li><p><strong>Traverse the Dataset Directory</strong>:</p>
<ul class="simple">
<li><p>Iterates through all files in the specified <code class="docutils literal notranslate"><span class="pre">directory</span></code> under the <code class="docutils literal notranslate"><span class="pre">bottle/</span></code> directory.</p></li>
<li><p>For each file, checks its name to determine its label:</p>
<ul>
<li><p>Files containing <code class="docutils literal notranslate"><span class="pre">&quot;good&quot;</span></code> are labeled as <code class="docutils literal notranslate"><span class="pre">&quot;good&quot;</span></code>.</p></li>
<li><p>Files containing <code class="docutils literal notranslate"><span class="pre">&quot;broken_large&quot;</span></code> are labeled as <code class="docutils literal notranslate"><span class="pre">&quot;broken_large&quot;</span></code>.</p></li>
<li><p>Files containing <code class="docutils literal notranslate"><span class="pre">&quot;broken_small&quot;</span></code> are labeled as <code class="docutils literal notranslate"><span class="pre">&quot;broken_small&quot;</span></code>.</p></li>
<li><p>Files containing <code class="docutils literal notranslate"><span class="pre">&quot;contamination&quot;</span></code> are labeled as <code class="docutils literal notranslate"><span class="pre">&quot;contamination&quot;</span></code>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Write JSON Output</strong>:</p>
<ul class="simple">
<li><p>Saves the <code class="docutils literal notranslate"><span class="pre">label_img_mapping</span></code> dictionary to the specified <code class="docutils literal notranslate"><span class="pre">output_file</span></code> in JSON format, with indented formatting for readability.</p></li>
</ul>
</li>
</ol>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="c1">## Need to create the json file for the DataLoader</span>

<span class="k">def</span> <span class="nf">create_json_labels</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">output_file</span><span class="p">):</span>
    <span class="c1">## remove an existing annotation file.</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_file</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">output_file</span><span class="p">)</span>

    <span class="c1">## Created from the ground truth folder</span>
    <span class="c1">## Key - filename (i.e img)</span>
    <span class="n">label_img_mapping</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="nb">dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;bottle&#39;</span><span class="p">,</span> <span class="n">directory</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">root</span><span class="p">,</span> <span class="n">dirs</span><span class="p">,</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="nb">dir</span><span class="p">):</span>
        
        <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;good&#39;</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">label_img_mapping</span><span class="p">[</span><span class="n">file</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;good&#39;</span>
            <span class="k">elif</span> <span class="s1">&#39;broken_large&#39;</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">label_img_mapping</span><span class="p">[</span><span class="n">file</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;broken_large&#39;</span>
            <span class="k">elif</span> <span class="s1">&#39;broken_small&#39;</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">label_img_mapping</span><span class="p">[</span><span class="n">file</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;broken_small&#39;</span>
            <span class="k">elif</span> <span class="s1">&#39;contamination&#39;</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">label_img_mapping</span><span class="p">[</span><span class="n">file</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;contamination&#39;</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">label_img_mapping</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>


<span class="c1">## Create the annotations for the training dataset - broken_large, broken_small, contamination, good</span>
<span class="n">create_json_labels</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;train_annotations.json&#39;</span><span class="p">)</span>            

<span class="c1">## Create the annotations for the test dataset - good</span>
<span class="n">create_json_labels</span><span class="p">(</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;test_annotations.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="pytorch-preparation-custom-dataset-and-dataloader-for-bottle-classification">
<h1>PyTorch Preparation: Custom Dataset and DataLoader for Bottle Classification<a class="headerlink" href="#pytorch-preparation-custom-dataset-and-dataloader-for-bottle-classification" title="Link to this heading">#</a></h1>
<section id="id3">
<h2>Functionality Overview<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>This code defines a custom PyTorch dataset class for loading and preprocessing images of bottles along with their labels. It utilizes the <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> interface and integrates data augmentation using <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code>. A DataLoader is used to batch and shuffle the dataset for training.</p>
</section>
<hr class="docutils" />
<section id="id4">
<h2>Code Details<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<section id="custombottledataset-class">
<h3><strong><code class="docutils literal notranslate"><span class="pre">CustomBottleDataset</span></code> Class</strong><a class="headerlink" href="#custombottledataset-class" title="Link to this heading">#</a></h3>
<p>A custom dataset class inheriting from PyTorchâ€™s <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> interface.</p>
<section id="initialization-init">
<h4><strong>Initialization (<code class="docutils literal notranslate"><span class="pre">__init__</span></code>)</strong>:<a class="headerlink" href="#initialization-init" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Parameters</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">data_directory</span></code>: The directory where image files are stored.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">labels_path</span></code>: Path to the JSON file containing image-label mappings.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transform</span></code>: Optional data transformations to apply to the images (default is <code class="docutils literal notranslate"><span class="pre">None</span></code>).</p></li>
</ul>
</li>
<li><p><strong>Functionality</strong>:</p>
<ul>
<li><p>Loads the label mapping from the JSON file using <code class="docutils literal notranslate"><span class="pre">json.load</span></code>.</p></li>
<li><p>Stores the directory and transformation for later use.</p></li>
</ul>
</li>
</ul>
</section>
<section id="length-len">
<h4><strong>Length (<code class="docutils literal notranslate"><span class="pre">__len__</span></code>)</strong>:<a class="headerlink" href="#length-len" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Returns the total number of images in the dataset (i.e., the number of entries in <code class="docutils literal notranslate"><span class="pre">image_labels</span></code>).</p></li>
</ul>
</section>
<section id="item-retrieval-getitem">
<h4><strong>Item Retrieval (<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>)</strong>:<a class="headerlink" href="#item-retrieval-getitem" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Parameters</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">index</span></code>: Index of the image to retrieve.</p></li>
</ul>
</li>
<li><p><strong>Process</strong>:</p>
<ol class="arabic simple">
<li><p>Retrieves the filename and label corresponding to the given index.</p></li>
<li><p>Constructs the full image path and reads the image using <code class="docutils literal notranslate"><span class="pre">torchvision.io.read_image</span></code>.</p></li>
<li><p>Normalizes the pixel values to the range [0, 1].</p></li>
<li><p>Applies transformations (if provided).</p></li>
<li><p>Maps the label (string) to an integer using a predefined dictionary:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">good</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">broken_small</span></code>: <code class="docutils literal notranslate"><span class="pre">1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">broken_large</span></code>: <code class="docutils literal notranslate"><span class="pre">2</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">contamination</span></code>: <code class="docutils literal notranslate"><span class="pre">3</span></code></p></li>
</ul>
</li>
<li><p>Returns the processed image and its corresponding label as a tuple <code class="docutils literal notranslate"><span class="pre">(img,</span> <span class="pre">label)</span></code>.</p></li>
</ol>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="transformations-t">
<h3><strong>Transformations (<code class="docutils literal notranslate"><span class="pre">T</span></code>)</strong><a class="headerlink" href="#transformations-t" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Purpose</strong>:</p>
<ul>
<li><p>Augment and normalize the images to prepare them for training.</p></li>
</ul>
</li>
<li><p><strong>Steps</strong> - (The transform (<code class="docutils literal notranslate"><span class="pre">T</span></code>) is used to create variability in the dataset because there is limited data for each class):</p>
<ol class="arabic simple">
<li><p><strong>Resize</strong>: Resizes all images to <code class="docutils literal notranslate"><span class="pre">(224,</span> <span class="pre">224)</span></code> with anti-aliasing.</p></li>
<li><p><strong>Random Rotation</strong>: Randomly rotates images up to 45 degrees.</p></li>
<li><p><strong>Random Horizontal Flip</strong>: Randomly flips images horizontally.</p></li>
<li><p><strong>Normalize</strong>: Normalizes the images using ImageNet mean and standard deviation values:</p>
<ul>
<li><p>Mean: <code class="docutils literal notranslate"><span class="pre">[0.485,</span> <span class="pre">0.456,</span> <span class="pre">0.406]</span></code></p></li>
<li><p>Std: <code class="docutils literal notranslate"><span class="pre">[0.229,</span> <span class="pre">0.224,</span> <span class="pre">0.225]</span></code></p></li>
</ul>
</li>
</ol>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="dataset-and-dataloader">
<h3><strong>Dataset and DataLoader</strong><a class="headerlink" href="#dataset-and-dataloader" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Dataset</strong>:</p>
<ul>
<li><p>Creates an instance of <code class="docutils literal notranslate"><span class="pre">CustomBottleDataset</span></code> for the training dataset using:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">data_directory='bottle/train'</span></code>: Directory containing training images.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">labels_path='train_annotations.json'</span></code>: Annotation file mapping filenames to labels.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transform=T</span></code>: Data augmentation and normalization transformations.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>DataLoader</strong>:</p>
<ul>
<li><p>Wraps the dataset in a PyTorch <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> for efficient batching and shuffling:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size=5</span></code>: Loads 5 images per batch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>: Randomly shuffles the dataset each epoch to improve training.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision.io</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">v2</span>


<span class="k">class</span> <span class="nc">CustomBottleDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_directory</span><span class="p">,</span> <span class="n">labels_path</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">labels_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">image_labels</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="n">data_directory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_labels</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_labels</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_labels</span><span class="p">[</span><span class="n">filename</span><span class="p">]</span>

        <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
            <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        
        <span class="n">label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;good&#39;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;broken_small&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;broken_large&#39;</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;contamination&#39;</span> <span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label_mapping</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">label</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">),</span> <span class="n">antialias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="n">degrees</span><span class="o">=</span><span class="mi">45</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
<span class="p">])</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CustomBottleDataset</span><span class="p">(</span><span class="n">data_directory</span><span class="o">=</span><span class="s1">&#39;bottle/train&#39;</span><span class="p">,</span> <span class="n">labels_path</span><span class="o">=</span><span class="s1">&#39;train_annotations.json&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 3, 224, 224]) tensor([0, 2, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 2, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 1, 2, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 2, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([3, 0, 1, 3, 1])
torch.Size([5, 3, 224, 224]) tensor([0, 1, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 1, 0])
torch.Size([5, 3, 224, 224]) tensor([2, 0, 2, 0, 2])
torch.Size([5, 3, 224, 224]) tensor([2, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 1])
torch.Size([5, 3, 224, 224]) tensor([3, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 2])
torch.Size([5, 3, 224, 224]) tensor([2, 0, 1, 0, 1])
torch.Size([5, 3, 224, 224]) tensor([3, 0, 0, 2, 0])
torch.Size([5, 3, 224, 224]) tensor([2, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 1, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([1, 0, 0, 0, 1])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 3])
torch.Size([5, 3, 224, 224]) tensor([3, 3, 0, 3, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 2])
torch.Size([5, 3, 224, 224]) tensor([3, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([1, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 1, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 2, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 3, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 3, 1])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 1, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 2, 0, 3, 0])
torch.Size([5, 3, 224, 224]) tensor([1, 0, 3, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 2, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 3, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 1, 1, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 2, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 3, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 3])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 3, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 3, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 0, 0])
torch.Size([5, 3, 224, 224]) tensor([0, 0, 0, 1, 3])
torch.Size([5, 3, 224, 224]) tensor([1, 0, 0, 0, 0])
torch.Size([1, 3, 224, 224]) tensor([2])
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="custom-cnn-using-reference-model-defect-identification-neural-network">
<h1>Custom CNN Using Reference Model: Defect Identification Neural Network<a class="headerlink" href="#custom-cnn-using-reference-model-defect-identification-neural-network" title="Link to this heading">#</a></h1>
<section id="id5">
<h2>Functionality Overview<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>This code defines a neural network architecture for defect identification based on the ResNet-18 model, pretrained on ImageNet. The network is customized for a 4-class classification task, where defects are classified into one of four categories, <code class="docutils literal notranslate"><span class="pre">good</span></code>, <code class="docutils literal notranslate"><span class="pre">broken</span> <span class="pre">small</span></code>, <code class="docutils literal notranslate"><span class="pre">broken</span> <span class="pre">large</span></code>, and <code class="docutils literal notranslate"><span class="pre">contamination</span></code>.</p>
</section>
<hr class="docutils" />
<section id="id6">
<h2>Code Details<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<section id="defectidentificationnetwork-class">
<h3><strong><code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> Class</strong><a class="headerlink" href="#defectidentificationnetwork-class" title="Link to this heading">#</a></h3>
<p>A custom PyTorch model class inheriting from <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p>
<section id="id7">
<h4><strong>Initialization (<code class="docutils literal notranslate"><span class="pre">__init__</span></code>)</strong>:<a class="headerlink" href="#id7" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Base Model</strong>:</p>
<ul>
<li><p>Uses a pretrained ResNet-18 model (<code class="docutils literal notranslate"><span class="pre">torchvision.models.resnet18</span></code> with <code class="docutils literal notranslate"><span class="pre">IMAGENET1K_V1</span></code> weights).</p></li>
<li><p>Extracts key layers from ResNet-18 to use in the custom architecture.</p></li>
</ul>
</li>
<li><p><strong>Layers Used from ResNet-18</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">conv1</span></code>: Initial convolution layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bn1</span></code>: Batch normalization after the first convolution.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">relu</span></code>: Activation function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">maxpool</span></code>: Max pooling operation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">layer1</span></code>, <code class="docutils literal notranslate"><span class="pre">layer2</span></code>, <code class="docutils literal notranslate"><span class="pre">layer3</span></code>, <code class="docutils literal notranslate"><span class="pre">layer4</span></code>: Sequential residual layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">avgpool</span></code>: Global average pooling layer.</p></li>
</ul>
</li>
<li><p><strong>Custom Fully Connected Layer</strong>:</p>
<ul>
<li><p>Replaces ResNet-18â€™s original fully connected (FC) layer with a new layer tailored for the 4-class classification task:</p>
<ul>
<li><p>Input size: 512 (output of ResNet-18â€™s <code class="docutils literal notranslate"><span class="pre">avgpool</span></code> layer).</p></li>
<li><p>Output size: 4 (number of classes).</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="forward-pass-forward">
<h4><strong>Forward Pass (<code class="docutils literal notranslate"><span class="pre">forward</span></code>)</strong>:<a class="headerlink" href="#forward-pass-forward" title="Link to this heading">#</a></h4>
<p>Defines the forward pass through the network:</p>
<ol class="arabic simple">
<li><p><strong>Feature Extraction</strong>:</p>
<ul class="simple">
<li><p>Input image is passed sequentially through the layers extracted from ResNet-18 (<code class="docutils literal notranslate"><span class="pre">conv1</span></code>, <code class="docutils literal notranslate"><span class="pre">bn1</span></code>, <code class="docutils literal notranslate"><span class="pre">relu</span></code>, <code class="docutils literal notranslate"><span class="pre">maxpool</span></code>, and <code class="docutils literal notranslate"><span class="pre">layer1</span></code> through <code class="docutils literal notranslate"><span class="pre">layer4</span></code>).</p></li>
<li><p>Outputs high-level feature representations of the input.</p></li>
</ul>
</li>
<li><p><strong>Global Average Pooling</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">avgpool</span></code> layer reduces the spatial dimensions, producing a compact feature vector.</p></li>
</ul>
</li>
<li><p><strong>Flattening</strong>:</p>
<ul class="simple">
<li><p>The output from <code class="docutils literal notranslate"><span class="pre">avgpool</span></code> is flattened into a 1D vector using <code class="docutils literal notranslate"><span class="pre">torch.flatten</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Classification</strong>:</p>
<ul class="simple">
<li><p>The flattened vector is passed through the custom fully connected layer (<code class="docutils literal notranslate"><span class="pre">fc</span></code>), producing logits for the 4 classes.</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">class</span> <span class="nc">DefectIdentificationNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DefectIdentificationNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">ref_model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ResNet18_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">conv1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">bn1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">relu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">maxpool</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">layer1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">layer2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">layer3</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">layer4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">avgpool</span>
        

        <span class="c1">## custom fc for # of classes i want to classify -&gt; currently at 512 (classifying 4 attribute)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="training-step-training-and-tracking-the-defect-identification-model">
<h1>Training Step: Training and Tracking the Defect Identification Model<a class="headerlink" href="#training-step-training-and-tracking-the-defect-identification-model" title="Link to this heading">#</a></h1>
<section id="id8">
<h2>Functionality Overview<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>This script handles the training of the <code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> model, including:</p>
<ul class="simple">
<li><p>Loading a pretrained model (if available).</p></li>
<li><p>Setting up the optimizer and loss function.</p></li>
<li><p>Training the model over multiple epochs with a PyTorch DataLoader.</p></li>
<li><p>Tracking and saving the training loss for visualization and reporting.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id9">
<h2>Code Details<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<section id="model-initialization-and-loading">
<h3><strong>1. Model Initialization and Loading</strong><a class="headerlink" href="#model-initialization-and-loading" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Model Setup</strong>:</p>
<ul class="simple">
<li><p>Instantiates the <code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> model.</p></li>
</ul>
</li>
<li><p><strong>Loading a Saved Model</strong>:</p>
<ul class="simple">
<li><p>Checks if a saved model (<code class="docutils literal notranslate"><span class="pre">base-model.pth</span></code>) exists in the <code class="docutils literal notranslate"><span class="pre">models/</span></code> directory.</p></li>
<li><p>If found, loads the model weights using <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Device Selection</strong>:</p>
<ul>
<li><p>Uses Apple Silicon (MPS) if available, otherwise defaults to CPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Could use GPU if available:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Moves the model to the selected device.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="training-configuration">
<h3><strong>2. Training Configuration</strong><a class="headerlink" href="#training-configuration" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Hyperparameters</strong>:</p>
<ul>
<li><p><strong>Learning Rate</strong>: (1 \times 10^{-4})</p></li>
<li><p><strong>Optimizer</strong>: Adam optimizer for adaptive learning rates.</p></li>
<li><p><strong>Loss Function</strong>: Cross-Entropy Loss, suitable for multi-class classification.</p></li>
<li><p><strong>Number of Epochs</strong>: 10.</p></li>
</ul>
</li>
<li><p><strong>Training Mode</strong>:</p>
<ul>
<li><p>The model is put into training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code>, enabling the model to update weights during backpropagation.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="training-loop">
<h3><strong>3. Training Loop</strong><a class="headerlink" href="#training-loop" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Loss Tracking</strong>:</p>
<ul class="simple">
<li><p>Initializes an empty list <code class="docutils literal notranslate"><span class="pre">loss_arr</span></code> to store average loss values for each epoch.</p></li>
</ul>
</li>
<li><p><strong>Per-Epoch Workflow</strong>:</p>
<ul class="simple">
<li><p>For each epoch:</p>
<ol class="arabic simple">
<li><p><strong>Batch Processing</strong>:</p>
<ul>
<li><p>Loads image batches from the <code class="docutils literal notranslate"><span class="pre">train_dataloader</span></code>.</p></li>
<li><p>Moves images and labels to the selected device.</p></li>
<li><p>Performs the forward pass, computes the loss, and updates model weights via backpropagation.</p></li>
</ul>
</li>
<li><p><strong>Logging</strong>:</p>
<ul>
<li><p>Prints loss every 10 batches to monitor progress.</p></li>
</ul>
</li>
<li><p><strong>Model Saving</strong>:</p>
<ul>
<li><p>Saves the model weights after processing each batch to <code class="docutils literal notranslate"><span class="pre">models/base-model.pth</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Epoch Loss Calculation</strong>:</p>
<ul>
<li><p>Computes the average loss for the epoch and appends it to <code class="docutils literal notranslate"><span class="pre">loss_arr</span></code>.</p></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Example Output for a Batch</strong>:</p>
<div class="highlight-plaintext notranslate"><div class="highlight"><pre><span></span>Epoch [1/10], Step [10/100], Loss: 0.253
</pre></div>
</div>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="loss-tracking-and-reporting">
<h3><strong>4. Loss Tracking and Reporting</strong><a class="headerlink" href="#loss-tracking-and-reporting" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Loss Tracking</strong>:</p>
<ul class="simple">
<li><p>If a loss tracker file (<code class="docutils literal notranslate"><span class="pre">loss_tracker/loss_per_epoch_batch_base.csv</span></code>) exists, it is read into a DataFrame.</p></li>
<li><p>The average loss for the training session is calculated and appended to the DataFrame.</p></li>
<li><p>Saves the updated DataFrame back to the CSV file.</p></li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul>
<li><p>Plots the training loss for each epoch and saves the plot as <code class="docutils literal notranslate"><span class="pre">Base-loss.png</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_arr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss Over Time - Base Images&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;Base-loss.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DefectIdentificationNetwork</span><span class="p">()</span>


<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;models/base-model.pth&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Using saved model.&#39;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;models/base-model.pth&#39;</span><span class="p">))</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Type of device used for training: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># Put network into training mode, where neural network weights can change</span>

<span class="n">loss_arr</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Print every 10 batches</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">], Step [</span><span class="si">{</span><span class="n">batch_idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;models/base-model.pth&#39;</span><span class="p">)</span>
        


    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
    <span class="n">loss_arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Average Loss after epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">loss_tracker_base_path</span> <span class="o">=</span> <span class="s2">&quot;reports/loss_tracker/loss_per_epoch_batch_base.csv&quot;</span>

<span class="n">base_loss_tracker</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">loss_tracker_base_path</span><span class="p">):</span>
    <span class="n">base_loss_tracker</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">loss_tracker_base_path</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">base_loss_tracker</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Loss&quot;</span> <span class="p">:</span> <span class="p">[]})</span>

<span class="c1">## loss tracker example format - &quot;Loss&quot; : [0.5, 0.3, 0.2]</span>
<span class="n">average_loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss_arr</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_epochs</span>
<span class="n">base_loss_tracker</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">base_loss_tracker</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Loss&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">average_loss</span><span class="p">]})],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">base_loss_tracker</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">loss_tracker_base_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_arr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss Over Time - Base Images&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;Base-loss.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using saved model.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/v4/d7x3sgjd0zs0jbb4ytx6w2gh0000gn/T/ipykernel_98743/997913202.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don&#39;t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(&#39;models/base-model.pth&#39;))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Type of device used for training: mps
Epoch [1/10], Step [10/54], Loss: 0.015476815402507782
Epoch [1/10], Step [20/54], Loss: 0.09637700021266937
Epoch [1/10], Step [30/54], Loss: 0.010205201804637909
Epoch [1/10], Step [40/54], Loss: 0.3339485824108124
Epoch [1/10], Step [50/54], Loss: 0.018925173208117485
Average Loss after epoch 1: 0.16599841471071597
Epoch [2/10], Step [10/54], Loss: 0.05113572999835014
Epoch [2/10], Step [20/54], Loss: 0.09948614984750748
Epoch [2/10], Step [30/54], Loss: 0.020505908876657486
Epoch [2/10], Step [40/54], Loss: 0.07549496740102768
Epoch [2/10], Step [50/54], Loss: 0.04987450689077377
Average Loss after epoch 2: 0.195571167687713
Epoch [3/10], Step [10/54], Loss: 0.3666532039642334
Epoch [3/10], Step [20/54], Loss: 0.022365767508745193
Epoch [3/10], Step [30/54], Loss: 0.04114387184381485
Epoch [3/10], Step [40/54], Loss: 0.030626943334937096
Epoch [3/10], Step [50/54], Loss: 0.048480499535799026
Average Loss after epoch 3: 0.08208353255220034
Epoch [4/10], Step [10/54], Loss: 0.028957661241292953
Epoch [4/10], Step [20/54], Loss: 0.03402107208967209
Epoch [4/10], Step [30/54], Loss: 0.004334492143243551
Epoch [4/10], Step [40/54], Loss: 0.016232645139098167
Epoch [4/10], Step [50/54], Loss: 0.11807063966989517
Average Loss after epoch 4: 0.11695909272896608
Epoch [5/10], Step [10/54], Loss: 0.014941510744392872
Epoch [5/10], Step [20/54], Loss: 0.1257106065750122
Epoch [5/10], Step [30/54], Loss: 0.17398853600025177
Epoch [5/10], Step [40/54], Loss: 0.05032845214009285
Epoch [5/10], Step [50/54], Loss: 0.03395169973373413
Average Loss after epoch 5: 0.10564629212711696
Epoch [6/10], Step [10/54], Loss: 0.039462409913539886
Epoch [6/10], Step [20/54], Loss: 0.023590940982103348
Epoch [6/10], Step [30/54], Loss: 0.0051186769269406796
Epoch [6/10], Step [40/54], Loss: 0.012541967444121838
Epoch [6/10], Step [50/54], Loss: 0.06088773161172867
Average Loss after epoch 6: 0.061610621068178224
Epoch [7/10], Step [10/54], Loss: 0.0629083663225174
Epoch [7/10], Step [20/54], Loss: 0.01763017289340496
Epoch [7/10], Step [30/54], Loss: 0.0029069208540022373
Epoch [7/10], Step [40/54], Loss: 0.011545998044312
Epoch [7/10], Step [50/54], Loss: 0.18118168413639069
Average Loss after epoch 7: 0.2347731559485611
Epoch [8/10], Step [10/54], Loss: 0.05499180033802986
Epoch [8/10], Step [20/54], Loss: 0.04843960702419281
Epoch [8/10], Step [30/54], Loss: 0.02858762815594673
Epoch [8/10], Step [40/54], Loss: 0.02049294486641884
Epoch [8/10], Step [50/54], Loss: 0.027584129944443703
Average Loss after epoch 8: 0.18709108979372238
Epoch [9/10], Step [10/54], Loss: 0.01853777840733528
Epoch [9/10], Step [20/54], Loss: 0.06612181663513184
Epoch [9/10], Step [30/54], Loss: 0.04531871899962425
Epoch [9/10], Step [40/54], Loss: 0.022271722555160522
Epoch [9/10], Step [50/54], Loss: 0.05640523508191109
Average Loss after epoch 9: 0.07174377467621255
Epoch [10/10], Step [10/54], Loss: 0.2257930040359497
Epoch [10/10], Step [20/54], Loss: 0.026627197861671448
Epoch [10/10], Step [30/54], Loss: 0.010176720097661018
Epoch [10/10], Step [40/54], Loss: 0.09010686725378036
Epoch [10/10], Step [50/54], Loss: 0.0042063347063958645
Average Loss after epoch 10: 0.1391140936649646
</pre></div>
</div>
<img alt="_images/b20bf784d6e503fc3c1714e56da6aecca90d5448b76f8eff090bc6f28a56e8c7.png" src="_images/b20bf784d6e503fc3c1714e56da6aecca90d5448b76f8eff090bc6f28a56e8c7.png" />
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="validation-step-model-evaluation-and-performance-reporting">
<h1>Validation Step: Model Evaluation and Performance Reporting<a class="headerlink" href="#validation-step-model-evaluation-and-performance-reporting" title="Link to this heading">#</a></h1>
<section id="id10">
<h2>Functionality Overview<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<p>This script evaluates the trained <code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> on a test dataset, generates predictions, calculates performance metrics (accuracy and misclassification rate), and saves classification results and confusion matrix data to files.</p>
</section>
<hr class="docutils" />
<section id="id11">
<h2>Code Details<a class="headerlink" href="#id11" title="Link to this heading">#</a></h2>
<section id="dataset-and-dataloader-setup">
<h3><strong>1. Dataset and DataLoader Setup</strong><a class="headerlink" href="#dataset-and-dataloader-setup" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Transforms</strong>:</p>
<ul>
<li><p>Applies a set of transformations to the test dataset:</p>
<ul>
<li><p>Resize images to <code class="docutils literal notranslate"><span class="pre">(224,</span> <span class="pre">224)</span></code>.</p></li>
<li><p>Randomly rotate images by up to 15 degrees.</p></li>
<li><p>Randomly horizontally flip images with a 90% probability.</p></li>
<li><p>Normalize using ImageNet mean and standard deviation values.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Test Dataset and DataLoader</strong>:</p>
<ul>
<li><p>Loads the <code class="docutils literal notranslate"><span class="pre">CustomBottleDataset</span></code> for the test set using:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">data_directory='bottle/test'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">labels_path='test_annotations.json'</span></code>.</p></li>
<li><p>Transformation pipeline (<code class="docutils literal notranslate"><span class="pre">T</span></code>).</p></li>
</ul>
</li>
<li><p>Wraps the dataset in a PyTorch <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> with a batch size of 5 and no shuffling.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="model-initialization">
<h3><strong>2. Model Initialization</strong><a class="headerlink" href="#model-initialization" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Loading Trained Model</strong>:</p>
<ul class="simple">
<li><p>Loads the weights of the previously trained model from <code class="docutils literal notranslate"><span class="pre">models/base-model.pth</span></code>.</p></li>
<li><p>Moves the model to the selected device (MPS for Apple Silicon if available, otherwise CPU).</p></li>
<li><p>Puts the model in evaluation mode using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Label Mapping</strong>:</p>
<ul>
<li><p>Uses a predefined mapping for labels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;good&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;broken-small&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;broken-large&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;contamination&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Creates a reverse mapping (<code class="docutils literal notranslate"><span class="pre">reverse_label_mapping</span></code>) for converting integer labels back to their string representations.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="evaluation-loop">
<h3><strong>3. Evaluation Loop</strong><a class="headerlink" href="#evaluation-loop" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Batch Processing</strong>:</p>
<ul>
<li><p>For each batch in the test DataLoader:</p>
<ol class="arabic simple">
<li><p>Performs a forward pass to compute predictions.</p></li>
<li><p>Uses <code class="docutils literal notranslate"><span class="pre">torch.max</span></code> to extract the predicted class for each image.</p></li>
<li><p>Appends predictions and ground truth labels to lists for later metric computation.</p></li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Logging Results</strong>:</p>
<ul>
<li><p>For each image in the batch:</p>
<ul>
<li><p>Logs the filename, true label, and predicted label to a text file (<code class="docutils literal notranslate"><span class="pre">classification_results.txt</span></code>).</p></li>
<li><p>Adds classification results to a confusion matrix mapping structure for later DataFrame creation.</p></li>
<li><p>Visualizes incorrectly classified images by reversing normalization and displaying the image using <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="metrics-calculation">
<h3><strong>4. Metrics Calculation</strong><a class="headerlink" href="#metrics-calculation" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Accuracy</strong>:</p>
<ul>
<li><p>Computes the classification accuracy using <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.accuracy_score</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">groundtruth</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><strong>Misclassification Rate</strong>:</p>
<ul>
<li><p>Constructs a DataFrame (<code class="docutils literal notranslate"><span class="pre">conf_df</span></code>) from the confusion matrix mapping.</p></li>
<li><p>Calculates the misclassification rate:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_false</span> <span class="o">=</span> <span class="p">(</span><span class="n">conf_df</span><span class="p">[</span><span class="s1">&#39;Result&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">misclassification_rate</span> <span class="o">=</span> <span class="n">num_false</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">conf_df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of false classifications:&quot;</span><span class="p">,</span> <span class="n">num_false</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Misclassification rate: %&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">misclassification_rate</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="output-files">
<h3><strong>5. Output Files</strong><a class="headerlink" href="#output-files" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Classification Results</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">classification_results.txt</span></code>: Logs filenames, true labels, and predicted labels for each test image.</p></li>
</ul>
</li>
<li><p><strong>Confusion Matrix Data</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">base_conf_matrix.csv</span></code>: Stores a detailed confusion matrix as a CSV with columns:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Image</span></code>: Filename of the test image.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Actual</span></code>: True label.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Predicted</span></code>: Predicted label.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Result</span></code>: Whether the prediction was correct (True/False).</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul>
<li><p>Displays incorrectly classified images with titles showing the filename, true label, and predicted label.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">cv2</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">),</span> <span class="n">antialias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="n">degrees</span><span class="o">=</span><span class="mi">15</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
<span class="p">])</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">CustomBottleDataset</span><span class="p">(</span><span class="n">data_directory</span><span class="o">=</span><span class="s1">&#39;bottle/test&#39;</span><span class="p">,</span> <span class="n">labels_path</span><span class="o">=</span><span class="s1">&#39;test_annotations.json&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">groundtruth</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DefectIdentificationNetwork</span><span class="p">()</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;models/base-model.pth&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Execute the previous cell to train the base model.&#39;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;models/base-model.pth&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Ensure the model is in evaluation mode</span>


<span class="c1"># Reverse the label mapping for easier interpretation</span>
<span class="n">label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;good&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;broken-small&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;broken-large&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;contamination&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">reverse_label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">label_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">filenames</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">test_dataset</span><span class="o">.</span><span class="n">image_labels</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="n">global_idx</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">conf_matrix_mapping</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Image&quot;</span> <span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;Actual&quot;</span> <span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;Predicted&quot;</span> <span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;Result&quot;</span> <span class="p">:</span> <span class="p">[]</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
    <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">)</span>

    <span class="c1"># Perform forward pass without disabling gradients</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">groundtruth</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)):</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">true_label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">pred_label</span> <span class="o">=</span> <span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Write classification results to the file</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;classification_results.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">log_file</span><span class="p">:</span>
            <span class="n">log_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Image: </span><span class="si">{</span><span class="n">filenames</span><span class="p">[</span><span class="n">global_idx</span><span class="p">]</span><span class="si">}</span><span class="s2">, True: </span><span class="si">{</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">true_label</span><span class="p">]</span><span class="si">}</span><span class="s2">, Pred: </span><span class="si">{</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">pred_label</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Reverse normalization for visualization</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">*</span> <span class="n">std</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">mean</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>

        <span class="c1"># Plot if prediction is incorrect</span>
        <span class="k">if</span> <span class="n">pred_label</span> <span class="o">!=</span> <span class="n">true_label</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Filename: </span><span class="si">{</span><span class="n">filenames</span><span class="p">[</span><span class="n">global_idx</span><span class="p">]</span><span class="si">}</span><span class="s2"> True: </span><span class="si">{</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">true_label</span><span class="p">]</span><span class="si">}</span><span class="s2">, Pred: </span><span class="si">{</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">pred_label</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        
        <span class="n">conf_matrix_mapping</span><span class="p">[</span><span class="s1">&#39;Image&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filenames</span><span class="p">[</span><span class="n">global_idx</span><span class="p">])</span>
        <span class="n">conf_matrix_mapping</span><span class="p">[</span><span class="s1">&#39;Actual&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">true_label</span><span class="p">])</span>
        <span class="n">conf_matrix_mapping</span><span class="p">[</span><span class="s1">&#39;Predicted&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">pred_label</span><span class="p">])</span>
        <span class="n">conf_matrix_mapping</span><span class="p">[</span><span class="s1">&#39;Result&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_label</span> <span class="o">==</span> <span class="n">true_label</span><span class="p">)</span>

        <span class="n">global_idx</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Increment global index</span>

<span class="c1"># Calculate accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">groundtruth</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>

<span class="n">conf_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">conf_matrix_mapping</span><span class="p">)</span>

<span class="n">report_path</span> <span class="o">=</span> <span class="s2">&quot;reports/base_conf_matrix.csv&quot;</span>

<span class="n">conf_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">report_path</span><span class="p">)</span>

<span class="c1"># Count the number of False results</span>
<span class="n">num_false</span> <span class="o">=</span> <span class="p">(</span><span class="n">conf_df</span><span class="p">[</span><span class="s1">&#39;Result&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># Total number of rows</span>
<span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">conf_df</span><span class="p">)</span>

<span class="c1"># Calculate misclassification rate</span>
<span class="n">misclassification_rate</span> <span class="o">=</span> <span class="n">num_false</span> <span class="o">/</span> <span class="n">total</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of false classifications:&quot;</span><span class="p">,</span> <span class="n">num_false</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Misclassification rate: %&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">misclassification_rate</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/v4/d7x3sgjd0zs0jbb4ytx6w2gh0000gn/T/ipykernel_98743/3184302117.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don&#39;t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(&#39;models/base-model.pth&#39;, map_location=device))
</pre></div>
</div>
<img alt="_images/88e9d69d659fd3814eaa11980cd31cfe7c947e87aa49f4165e6860c1894cadce.png" src="_images/88e9d69d659fd3814eaa11980cd31cfe7c947e87aa49f4165e6860c1894cadce.png" />
<img alt="_images/3f6fd96ca598d2ea459d9915179fdb6f8282941f199de9b045e30fb1434e0980.png" src="_images/3f6fd96ca598d2ea459d9915179fdb6f8282941f199de9b045e30fb1434e0980.png" />
<img alt="_images/03ddc006760862af681923c3be4762fffbb40e58b107daf2c27d0bfd9308bb11.png" src="_images/03ddc006760862af681923c3be4762fffbb40e58b107daf2c27d0bfd9308bb11.png" />
<img alt="_images/e868a6c3820f0ec67ca3faf014706a9b4703d4e5162c25b1027af10bc07beacd.png" src="_images/e868a6c3820f0ec67ca3faf014706a9b4703d4e5162c25b1027af10bc07beacd.png" />
<img alt="_images/711693d1f2995902566120ed1a85a6fad769777620674206bac23f021c0cb416.png" src="_images/711693d1f2995902566120ed1a85a6fad769777620674206bac23f021c0cb416.png" />
<img alt="_images/0ddee78ef56426232b57caa2bc75213a05092e2ee599dca8b82b5220384d9670.png" src="_images/0ddee78ef56426232b57caa2bc75213a05092e2ee599dca8b82b5220384d9670.png" />
<img alt="_images/e7366c0524097440cc6f15bdd533ce5c449e08ee51bba0296cf5924f6e96fd53.png" src="_images/e7366c0524097440cc6f15bdd533ce5c449e08ee51bba0296cf5924f6e96fd53.png" />
<img alt="_images/fba9eb48b7409d2b64529d248e33b5162d0ff1f8f261fe0a156e901a72963ab9.png" src="_images/fba9eb48b7409d2b64529d248e33b5162d0ff1f8f261fe0a156e901a72963ab9.png" />
<img alt="_images/25ddacda1f0e8008968db9e2f7caa83a4430c4347388ab55f69e330682569629.png" src="_images/25ddacda1f0e8008968db9e2f7caa83a4430c4347388ab55f69e330682569629.png" />
<img alt="_images/84de0c33afd6f4fd73969909653d359cb2b99ffcd0a4062502da21ae1bda394d.png" src="_images/84de0c33afd6f4fd73969909653d359cb2b99ffcd0a4062502da21ae1bda394d.png" />
<img alt="_images/679ce3ada74f86115dbf770b2dd6e540d57c80eced1f9ccce20827e6fc44bbf4.png" src="_images/679ce3ada74f86115dbf770b2dd6e540d57c80eced1f9ccce20827e6fc44bbf4.png" />
<img alt="_images/67591b5a819bbe420fae7079fe93381ffa06717f2a504b1ea7c85dbf89486aca.png" src="_images/67591b5a819bbe420fae7079fe93381ffa06717f2a504b1ea7c85dbf89486aca.png" />
<img alt="_images/3d6bb7aa107075c641b2826dedfb3c2728fbe1412a7ec45b83ff6255b67dac38.png" src="_images/3d6bb7aa107075c641b2826dedfb3c2728fbe1412a7ec45b83ff6255b67dac38.png" />
<img alt="_images/033bb77ea307bcb84be632b726523531c307ccedd6f51ef66286e6bd1fbf31bb.png" src="_images/033bb77ea307bcb84be632b726523531c307ccedd6f51ef66286e6bd1fbf31bb.png" />
<img alt="_images/7387cd77c5f67744f12470f2a9db52e9817c3955fdd662573ceb1327f81cf9f5.png" src="_images/7387cd77c5f67744f12470f2a9db52e9817c3955fdd662573ceb1327f81cf9f5.png" />
<img alt="_images/f30ac289f6d4199e320af3d833e4c531591d5286e48e1e0dd77d7fbf5143e488.png" src="_images/f30ac289f6d4199e320af3d833e4c531591d5286e48e1e0dd77d7fbf5143e488.png" />
<img alt="_images/851e4abb4b8849eb58953536367d1a6fa2b0148b2177e2294709a5e7df8119ab.png" src="_images/851e4abb4b8849eb58953536367d1a6fa2b0148b2177e2294709a5e7df8119ab.png" />
<img alt="_images/78cea49dae7f5c96bf23f3e64ba8f42aebb6945ebdf34342739568dd5909be1f.png" src="_images/78cea49dae7f5c96bf23f3e64ba8f42aebb6945ebdf34342739568dd5909be1f.png" />
<img alt="_images/bc376d25c4d2a9a11d1d8eee0f7dc7a3038b81365d123969d26143671c44ce2f.png" src="_images/bc376d25c4d2a9a11d1d8eee0f7dc7a3038b81365d123969d26143671c44ce2f.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 77.10843373493977%
Number of false classifications: 19
Misclassification rate: % 22.89156626506024
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="preprocessing-for-model-using-sharpened-images-sharpening-images-and-generating-annotations">
<h1>Preprocessing for Model Using Sharpened Images: Sharpening Images and Generating Annotations<a class="headerlink" href="#preprocessing-for-model-using-sharpened-images-sharpening-images-and-generating-annotations" title="Link to this heading">#</a></h1>
<section id="id12">
<h2>Functionality Overview<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<p>This script processes training and testing images by applying sharpening techniques using Sobel edge detection and overlaying the edges onto the original images. The processed images are saved to a designated directory, and annotation files for these sharpened images are generated.</p>
</section>
<hr class="docutils" />
<section id="id13">
<h2>Code Details<a class="headerlink" href="#id13" title="Link to this heading">#</a></h2>
<section id="sharpening-images">
<h3><strong>1. Sharpening Images</strong><a class="headerlink" href="#sharpening-images" title="Link to this heading">#</a></h3>
<p>The function <code class="docutils literal notranslate"><span class="pre">sharpen_images(base_dir,</span> <span class="pre">new_dir)</span></code> performs the following steps:</p>
<section id="input-parameters">
<h4><strong>Input Parameters</strong>:<a class="headerlink" href="#input-parameters" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">base_dir</span></code>: Directory containing the original images to be processed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">new_dir</span></code>: Directory to save the sharpened images.</p></li>
</ul>
</section>
<section id="id14">
<h4><strong>Steps</strong>:<a class="headerlink" href="#id14" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Ensure Output Directory</strong>:</p>
<ul class="simple">
<li><p>Creates the <code class="docutils literal notranslate"><span class="pre">new_dir</span></code> if it does not already exist.</p></li>
</ul>
</li>
<li><p><strong>Iterate Through Images</strong>:</p>
<ul class="simple">
<li><p>Walks through all image files in the <code class="docutils literal notranslate"><span class="pre">base_dir</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Image Processing</strong>:</p>
<ul class="simple">
<li><p>Reads each image using OpenCV.</p></li>
<li><p>Converts the image to grayscale for edge detection.</p></li>
<li><p>Applies Sobel filtering in both (x)- and (y)-directions to detect edges.</p></li>
<li><p>Calculates the edge magnitude and normalizes it to the range [0, 255].</p></li>
<li><p>Converts the edge map to a 3-channel image for overlaying.</p></li>
<li><p>Overlays the edges onto the original image using weighted addition to produce a sharpened effect.</p></li>
</ul>
</li>
<li><p><strong>Save Sharpened Images</strong>:</p>
<ul class="simple">
<li><p>Saves the processed image to <code class="docutils literal notranslate"><span class="pre">new_dir</span></code> with the same filename.</p></li>
</ul>
</li>
<li><p><strong>Logging</strong>:</p>
<ul class="simple">
<li><p>Logs the total number of images successfully processed.</p></li>
</ul>
</li>
</ol>
</section>
<section id="output">
<h4><strong>Output</strong>:<a class="headerlink" href="#output" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Sharpened images saved to <code class="docutils literal notranslate"><span class="pre">new_dir</span></code>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="generating-sharpened-image-annotations">
<h3><strong>2. Generating Sharpened Image Annotations</strong><a class="headerlink" href="#generating-sharpened-image-annotations" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Purpose</strong>:</p>
<ul class="simple">
<li><p>Generates annotation files for the sharpened images in both the training and testing datasets.</p></li>
</ul>
</li>
<li><p><strong>Function Call</strong>:</p>
<ul>
<li><p>Uses the previously defined <code class="docutils literal notranslate"><span class="pre">create_json_labels</span></code> function to generate JSON files mapping filenames to labels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">create_json_labels</span><span class="p">(</span><span class="s1">&#39;sharpened/train&#39;</span><span class="p">,</span> <span class="s1">&#39;bottle/sharpened/train_annotations.json&#39;</span><span class="p">)</span>
<span class="n">create_json_labels</span><span class="p">(</span><span class="s1">&#39;sharpened/test&#39;</span><span class="p">,</span> <span class="s1">&#39;bottle/sharpened/test_annotations.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="directory-structure">
<h3><strong>3. Directory Structure</strong><a class="headerlink" href="#directory-structure" title="Link to this heading">#</a></h3>
<p>The code block works with the following directory hierarchy:</p>
<ul class="simple">
<li><p><strong>Input Directories</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">bottle/train/</span></code>: Original training images.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bottle/test/</span></code>: Original testing images.</p></li>
</ul>
</li>
<li><p><strong>Output Directories</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">bottle/sharpened/train/</span></code>: Sharpened training images.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bottle/sharpened/test/</span></code>: Sharpened testing images.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Sharpening images </span>

<span class="kn">import</span> <span class="nn">cv2</span>


<span class="c1">## sharpening all training images and moving them to sharpened directory</span>

<span class="k">def</span> <span class="nf">sharpen_images</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="n">new_dir</span><span class="p">):</span>
    <span class="c1"># Ensure the output directory exists</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">new_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Sobel kernel will be used for edge detection</span>
    <span class="n">ct</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Counter for processed images</span>

    <span class="k">for</span> <span class="n">root</span><span class="p">,</span> <span class="n">dirs</span><span class="p">,</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="n">base_dir</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
            <span class="n">base_image_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">base_image_path</span><span class="p">):</span>
                <span class="c1"># Load the original color image</span>
                <span class="n">base_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">base_image_path</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">base_img</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not read the image at path: </span><span class="si">{</span><span class="n">base_image_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">continue</span>

                <span class="c1"># Convert the color image to grayscale for edge detection</span>
                <span class="n">base_img_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">base_img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

                <span class="c1"># Apply Sobel filtering to detect edges</span>
                <span class="n">sobel_x</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">base_img_gray</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CV_64F</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
                <span class="n">sobel_y</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">base_img_gray</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CV_64F</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
                <span class="n">edges</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">magnitude</span><span class="p">(</span><span class="n">sobel_x</span><span class="p">,</span> <span class="n">sobel_y</span><span class="p">)</span>

                <span class="c1"># Normalize edges to the range [0, 255] and convert to uint8</span>
                <span class="n">edges</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">NORM_MINMAX</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

                <span class="c1"># Convert edges to 3-channel format for overlay</span>
                <span class="n">edges_3channel</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_GRAY2BGR</span><span class="p">)</span>

                <span class="c1"># Overlay the edges onto the original color image</span>
                <span class="n">sharpened_color_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">addWeighted</span><span class="p">(</span><span class="n">base_img</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">edges_3channel</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Save the sharpened image</span>
                <span class="n">sharpened_img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">new_dir</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
                <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">sharpened_img_path</span><span class="p">,</span> <span class="n">sharpened_color_img</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved sharpened image to: </span><span class="si">{</span><span class="n">sharpened_img_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">ct</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Base image path does not exist: </span><span class="si">{</span><span class="n">base_image_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sharpened </span><span class="si">{</span><span class="n">ct</span><span class="si">}</span><span class="s2"> images&quot;</span><span class="p">)</span>


<span class="c1"># Directories for training and testing data</span>
<span class="n">base_training_directory</span> <span class="o">=</span> <span class="s1">&#39;bottle/train/&#39;</span>
<span class="n">sharpened_training_directory</span> <span class="o">=</span> <span class="s1">&#39;bottle/sharpened/train&#39;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">sharpened_training_directory</span><span class="p">):</span>
    <span class="n">sharpen_images</span><span class="p">(</span><span class="n">base_training_directory</span><span class="p">,</span> <span class="n">sharpened_training_directory</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Skipping creating training images.&#39;</span><span class="p">)</span>

<span class="n">base_test_directory</span> <span class="o">=</span> <span class="s1">&#39;bottle/test/&#39;</span>
<span class="n">sharpened_test_directory</span> <span class="o">=</span> <span class="s1">&#39;bottle/sharpened/test&#39;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">sharpened_test_directory</span><span class="p">):</span>
    <span class="n">sharpen_images</span><span class="p">(</span><span class="n">base_test_directory</span><span class="p">,</span> <span class="n">sharpened_test_directory</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Skipping creating testing images.&#39;</span><span class="p">)</span>


<span class="c1">## create the annotation files for the sharpened images.</span>
<span class="n">create_json_labels</span><span class="p">(</span><span class="s1">&#39;sharpened/train&#39;</span><span class="p">,</span> <span class="s1">&#39;bottle/sharpened/train_annotations.json&#39;</span><span class="p">)</span>            

<span class="c1">## Create the annotations for the test dataset - good</span>
<span class="n">create_json_labels</span><span class="p">(</span><span class="s1">&#39;sharpened/test&#39;</span><span class="p">,</span> <span class="s1">&#39;bottle/sharpened/test_annotations.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saved sharpened image to: bottle/sharpened/train/good_206.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_4.jpg
Saved sharpened image to: bottle/sharpened/train/good_5.jpg
Saved sharpened image to: bottle/sharpened/train/good_158.jpg
Saved sharpened image to: bottle/sharpened/train/good_170.jpg
Saved sharpened image to: bottle/sharpened/train/good_63.jpg
Saved sharpened image to: bottle/sharpened/train/good_77.jpg
Saved sharpened image to: bottle/sharpened/train/good_164.jpg
Saved sharpened image to: bottle/sharpened/train/good_88.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_8.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_5.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_4.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_9.jpg
Saved sharpened image to: bottle/sharpened/train/good_89.jpg
Saved sharpened image to: bottle/sharpened/train/good_165.jpg
Saved sharpened image to: bottle/sharpened/train/good_76.jpg
Saved sharpened image to: bottle/sharpened/train/good_62.jpg
Saved sharpened image to: bottle/sharpened/train/good_171.jpg
Saved sharpened image to: bottle/sharpened/train/good_159.jpg
Saved sharpened image to: bottle/sharpened/train/good_4.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_5.jpg
Saved sharpened image to: bottle/sharpened/train/good_207.jpg
Saved sharpened image to: bottle/sharpened/train/good_205.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_7.jpg
Saved sharpened image to: bottle/sharpened/train/good_6.jpg
Saved sharpened image to: bottle/sharpened/train/good_48.jpg
Saved sharpened image to: bottle/sharpened/train/good_74.jpg
Saved sharpened image to: bottle/sharpened/train/good_167.jpg
Saved sharpened image to: bottle/sharpened/train/good_173.jpg
Saved sharpened image to: bottle/sharpened/train/good_60.jpg
Saved sharpened image to: bottle/sharpened/train/good_198.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_19.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_6.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_7.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_18.jpg
Saved sharpened image to: bottle/sharpened/train/good_199.jpg
Saved sharpened image to: bottle/sharpened/train/good_61.jpg
Saved sharpened image to: bottle/sharpened/train/good_172.jpg
Saved sharpened image to: bottle/sharpened/train/good_166.jpg
Saved sharpened image to: bottle/sharpened/train/good_75.jpg
Saved sharpened image to: bottle/sharpened/train/good_49.jpg
Saved sharpened image to: bottle/sharpened/train/good_7.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_6.jpg
Saved sharpened image to: bottle/sharpened/train/good_204.jpg
Saved sharpened image to: bottle/sharpened/train/good_200.jpg
Saved sharpened image to: bottle/sharpened/train/good_3.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_2.jpg
Saved sharpened image to: bottle/sharpened/train/good_71.jpg
Saved sharpened image to: bottle/sharpened/train/good_162.jpg
Saved sharpened image to: bottle/sharpened/train/good_176.jpg
Saved sharpened image to: bottle/sharpened/train/good_65.jpg
Saved sharpened image to: bottle/sharpened/train/good_59.jpg
Saved sharpened image to: bottle/sharpened/train/good_189.jpg
Could not read the image at path: bottle/train/.DS_Store
Saved sharpened image to: bottle/sharpened/train/broken_small_3.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_2.jpg
Saved sharpened image to: bottle/sharpened/train/good_188.jpg
Saved sharpened image to: bottle/sharpened/train/good_58.jpg
Saved sharpened image to: bottle/sharpened/train/good_64.jpg
Saved sharpened image to: bottle/sharpened/train/good_177.jpg
Saved sharpened image to: bottle/sharpened/train/good_163.jpg
Saved sharpened image to: bottle/sharpened/train/good_70.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_3.jpg
Saved sharpened image to: bottle/sharpened/train/good_2.jpg
Saved sharpened image to: bottle/sharpened/train/good_201.jpg
Saved sharpened image to: bottle/sharpened/train/good_203.jpg
Saved sharpened image to: bottle/sharpened/train/good_0.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_1.jpg
Saved sharpened image to: bottle/sharpened/train/good_175.jpg
Saved sharpened image to: bottle/sharpened/train/good_66.jpg
Saved sharpened image to: bottle/sharpened/train/good_72.jpg
Saved sharpened image to: bottle/sharpened/train/good_161.jpg
Saved sharpened image to: bottle/sharpened/train/good_149.jpg
Saved sharpened image to: bottle/sharpened/train/good_99.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_0.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_1.jpg
Saved sharpened image to: bottle/sharpened/train/good_98.jpg
Saved sharpened image to: bottle/sharpened/train/good_148.jpg
Saved sharpened image to: bottle/sharpened/train/good_160.jpg
Saved sharpened image to: bottle/sharpened/train/good_73.jpg
Saved sharpened image to: bottle/sharpened/train/good_67.jpg
Saved sharpened image to: bottle/sharpened/train/good_174.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_0.jpg
Saved sharpened image to: bottle/sharpened/train/good_1.jpg
Saved sharpened image to: bottle/sharpened/train/good_202.jpg
Saved sharpened image to: bottle/sharpened/train/good_28.jpg
Saved sharpened image to: bottle/sharpened/train/good_113.jpg
Saved sharpened image to: bottle/sharpened/train/good_107.jpg
Saved sharpened image to: bottle/sharpened/train/good_14.jpg
Saved sharpened image to: bottle/sharpened/train/good_15.jpg
Saved sharpened image to: bottle/sharpened/train/good_106.jpg
Saved sharpened image to: bottle/sharpened/train/good_112.jpg
Saved sharpened image to: bottle/sharpened/train/good_29.jpg
Saved sharpened image to: bottle/sharpened/train/good_138.jpg
Saved sharpened image to: bottle/sharpened/train/good_104.jpg
Saved sharpened image to: bottle/sharpened/train/good_17.jpg
Saved sharpened image to: bottle/sharpened/train/good_110.jpg
Saved sharpened image to: bottle/sharpened/train/good_111.jpg
Saved sharpened image to: bottle/sharpened/train/good_16.jpg
Saved sharpened image to: bottle/sharpened/train/good_105.jpg
Saved sharpened image to: bottle/sharpened/train/good_139.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_18.jpg
Saved sharpened image to: bottle/sharpened/train/good_101.jpg
Saved sharpened image to: bottle/sharpened/train/good_12.jpg
Saved sharpened image to: bottle/sharpened/train/good_115.jpg
Saved sharpened image to: bottle/sharpened/train/good_129.jpg
Saved sharpened image to: bottle/sharpened/train/good_128.jpg
Saved sharpened image to: bottle/sharpened/train/good_114.jpg
Saved sharpened image to: bottle/sharpened/train/good_13.jpg
Saved sharpened image to: bottle/sharpened/train/good_100.jpg
Saved sharpened image to: bottle/sharpened/train/good_116.jpg
Saved sharpened image to: bottle/sharpened/train/good_102.jpg
Saved sharpened image to: bottle/sharpened/train/good_11.jpg
Saved sharpened image to: bottle/sharpened/train/good_39.jpg
Saved sharpened image to: bottle/sharpened/train/good_38.jpg
Saved sharpened image to: bottle/sharpened/train/good_10.jpg
Saved sharpened image to: bottle/sharpened/train/good_103.jpg
Saved sharpened image to: bottle/sharpened/train/good_117.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_17.jpg
Saved sharpened image to: bottle/sharpened/train/good_132.jpg
Saved sharpened image to: bottle/sharpened/train/good_21.jpg
Saved sharpened image to: bottle/sharpened/train/good_35.jpg
Saved sharpened image to: bottle/sharpened/train/good_126.jpg
Saved sharpened image to: bottle/sharpened/train/good_127.jpg
Saved sharpened image to: bottle/sharpened/train/good_34.jpg
Saved sharpened image to: bottle/sharpened/train/good_20.jpg
Saved sharpened image to: bottle/sharpened/train/good_133.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_16.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_14.jpg
Saved sharpened image to: bottle/sharpened/train/good_119.jpg
Saved sharpened image to: bottle/sharpened/train/good_36.jpg
Saved sharpened image to: bottle/sharpened/train/good_125.jpg
Saved sharpened image to: bottle/sharpened/train/good_131.jpg
Saved sharpened image to: bottle/sharpened/train/good_22.jpg
Saved sharpened image to: bottle/sharpened/train/good_23.jpg
Saved sharpened image to: bottle/sharpened/train/good_130.jpg
Saved sharpened image to: bottle/sharpened/train/good_124.jpg
Saved sharpened image to: bottle/sharpened/train/good_37.jpg
Saved sharpened image to: bottle/sharpened/train/good_118.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_15.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_11.jpg
Saved sharpened image to: bottle/sharpened/train/good_33.jpg
Saved sharpened image to: bottle/sharpened/train/good_120.jpg
Saved sharpened image to: bottle/sharpened/train/good_134.jpg
Saved sharpened image to: bottle/sharpened/train/good_27.jpg
Saved sharpened image to: bottle/sharpened/train/good_108.jpg
Saved sharpened image to: bottle/sharpened/train/good_109.jpg
Saved sharpened image to: bottle/sharpened/train/good_26.jpg
Saved sharpened image to: bottle/sharpened/train/good_135.jpg
Saved sharpened image to: bottle/sharpened/train/good_121.jpg
Saved sharpened image to: bottle/sharpened/train/good_32.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_10.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_12.jpg
Saved sharpened image to: bottle/sharpened/train/good_137.jpg
Saved sharpened image to: bottle/sharpened/train/good_24.jpg
Saved sharpened image to: bottle/sharpened/train/good_30.jpg
Saved sharpened image to: bottle/sharpened/train/good_123.jpg
Saved sharpened image to: bottle/sharpened/train/good_18.jpg
Saved sharpened image to: bottle/sharpened/train/good_19.jpg
Saved sharpened image to: bottle/sharpened/train/good_122.jpg
Saved sharpened image to: bottle/sharpened/train/good_31.jpg
Saved sharpened image to: bottle/sharpened/train/good_25.jpg
Saved sharpened image to: bottle/sharpened/train/good_136.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_13.jpg
Saved sharpened image to: bottle/sharpened/train/good_179.jpg
Saved sharpened image to: bottle/sharpened/train/good_42.jpg
Saved sharpened image to: bottle/sharpened/train/good_151.jpg
Saved sharpened image to: bottle/sharpened/train/good_145.jpg
Saved sharpened image to: bottle/sharpened/train/good_56.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_1.jpg
Saved sharpened image to: bottle/sharpened/train/good_192.jpg
Saved sharpened image to: bottle/sharpened/train/good_81.jpg
Saved sharpened image to: bottle/sharpened/train/good_95.jpg
Saved sharpened image to: bottle/sharpened/train/good_186.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_16.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_13.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_12.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_17.jpg
Saved sharpened image to: bottle/sharpened/train/good_187.jpg
Saved sharpened image to: bottle/sharpened/train/good_94.jpg
Saved sharpened image to: bottle/sharpened/train/good_80.jpg
Saved sharpened image to: bottle/sharpened/train/good_193.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_0.jpg
Saved sharpened image to: bottle/sharpened/train/good_57.jpg
Saved sharpened image to: bottle/sharpened/train/good_144.jpg
Saved sharpened image to: bottle/sharpened/train/good_150.jpg
Saved sharpened image to: bottle/sharpened/train/good_43.jpg
Saved sharpened image to: bottle/sharpened/train/good_178.jpg
Saved sharpened image to: bottle/sharpened/train/good_69.jpg
Saved sharpened image to: bottle/sharpened/train/good_146.jpg
Saved sharpened image to: bottle/sharpened/train/good_55.jpg
Saved sharpened image to: bottle/sharpened/train/good_41.jpg
Saved sharpened image to: bottle/sharpened/train/good_152.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_2.jpg
Saved sharpened image to: bottle/sharpened/train/good_96.jpg
Saved sharpened image to: bottle/sharpened/train/good_185.jpg
Saved sharpened image to: bottle/sharpened/train/good_191.jpg
Saved sharpened image to: bottle/sharpened/train/good_82.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_15.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_10.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_11.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_14.jpg
Saved sharpened image to: bottle/sharpened/train/good_83.jpg
Saved sharpened image to: bottle/sharpened/train/good_190.jpg
Saved sharpened image to: bottle/sharpened/train/good_184.jpg
Saved sharpened image to: bottle/sharpened/train/good_97.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_3.jpg
Saved sharpened image to: bottle/sharpened/train/good_153.jpg
Saved sharpened image to: bottle/sharpened/train/good_40.jpg
Saved sharpened image to: bottle/sharpened/train/good_54.jpg
Saved sharpened image to: bottle/sharpened/train/good_147.jpg
Saved sharpened image to: bottle/sharpened/train/good_68.jpg
Saved sharpened image to: bottle/sharpened/train/good_143.jpg
Saved sharpened image to: bottle/sharpened/train/good_50.jpg
Saved sharpened image to: bottle/sharpened/train/good_44.jpg
Saved sharpened image to: bottle/sharpened/train/good_157.jpg
Saved sharpened image to: bottle/sharpened/train/good_78.jpg
Saved sharpened image to: bottle/sharpened/train/good_93.jpg
Saved sharpened image to: bottle/sharpened/train/good_180.jpg
Saved sharpened image to: bottle/sharpened/train/good_194.jpg
Saved sharpened image to: bottle/sharpened/train/good_87.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_7.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_15.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_10.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_11.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_14.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_6.jpg
Saved sharpened image to: bottle/sharpened/train/good_86.jpg
Saved sharpened image to: bottle/sharpened/train/good_195.jpg
Saved sharpened image to: bottle/sharpened/train/good_181.jpg
Saved sharpened image to: bottle/sharpened/train/good_92.jpg
Saved sharpened image to: bottle/sharpened/train/good_79.jpg
Saved sharpened image to: bottle/sharpened/train/good_156.jpg
Saved sharpened image to: bottle/sharpened/train/good_45.jpg
Saved sharpened image to: bottle/sharpened/train/good_51.jpg
Saved sharpened image to: bottle/sharpened/train/good_142.jpg
Saved sharpened image to: bottle/sharpened/train/good_208.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_8.jpg
Saved sharpened image to: bottle/sharpened/train/good_9.jpg
Saved sharpened image to: bottle/sharpened/train/good_47.jpg
Saved sharpened image to: bottle/sharpened/train/good_154.jpg
Saved sharpened image to: bottle/sharpened/train/good_140.jpg
Saved sharpened image to: bottle/sharpened/train/good_53.jpg
Saved sharpened image to: bottle/sharpened/train/good_168.jpg
Saved sharpened image to: bottle/sharpened/train/good_197.jpg
Saved sharpened image to: bottle/sharpened/train/good_84.jpg
Saved sharpened image to: bottle/sharpened/train/good_90.jpg
Saved sharpened image to: bottle/sharpened/train/good_183.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_4.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_16.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_9.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_13.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_12.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_8.jpg
Saved sharpened image to: bottle/sharpened/train/broken_small_17.jpg
Saved sharpened image to: bottle/sharpened/train/broken_large_5.jpg
Saved sharpened image to: bottle/sharpened/train/good_182.jpg
Saved sharpened image to: bottle/sharpened/train/good_91.jpg
Saved sharpened image to: bottle/sharpened/train/good_85.jpg
Saved sharpened image to: bottle/sharpened/train/good_196.jpg
Saved sharpened image to: bottle/sharpened/train/good_169.jpg
Saved sharpened image to: bottle/sharpened/train/good_52.jpg
Saved sharpened image to: bottle/sharpened/train/good_141.jpg
Saved sharpened image to: bottle/sharpened/train/good_155.jpg
Saved sharpened image to: bottle/sharpened/train/good_46.jpg
Saved sharpened image to: bottle/sharpened/train/good_8.jpg
Saved sharpened image to: bottle/sharpened/train/contamination_9.jpg
Sharpened 266 images
Saved sharpened image to: bottle/sharpened/test/contamination_4.jpg
Saved sharpened image to: bottle/sharpened/test/good_5.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_8.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_5.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_4.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_9.jpg
Saved sharpened image to: bottle/sharpened/test/good_4.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_5.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_7.jpg
Saved sharpened image to: bottle/sharpened/test/good_6.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_19.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_6.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_7.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_18.jpg
Saved sharpened image to: bottle/sharpened/test/good_7.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_6.jpg
Saved sharpened image to: bottle/sharpened/test/good_3.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_2.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_20.jpg
Could not read the image at path: bottle/test/.DS_Store
Saved sharpened image to: bottle/sharpened/test/broken_large_19.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_3.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_2.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_21.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_18.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_3.jpg
Saved sharpened image to: bottle/sharpened/test/good_2.jpg
Saved sharpened image to: bottle/sharpened/test/good_0.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_1.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_0.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_1.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_0.jpg
Saved sharpened image to: bottle/sharpened/test/good_1.jpg
Saved sharpened image to: bottle/sharpened/test/good_14.jpg
Saved sharpened image to: bottle/sharpened/test/good_15.jpg
Saved sharpened image to: bottle/sharpened/test/good_17.jpg
Saved sharpened image to: bottle/sharpened/test/good_16.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_20.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_18.jpg
Saved sharpened image to: bottle/sharpened/test/good_12.jpg
Saved sharpened image to: bottle/sharpened/test/good_13.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_19.jpg
Saved sharpened image to: bottle/sharpened/test/good_11.jpg
Saved sharpened image to: bottle/sharpened/test/good_10.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_17.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_16.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_14.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_15.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_11.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_10.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_12.jpg
Saved sharpened image to: bottle/sharpened/test/good_18.jpg
Saved sharpened image to: bottle/sharpened/test/good_19.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_13.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_1.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_16.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_13.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_12.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_17.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_0.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_2.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_15.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_10.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_11.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_14.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_3.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_7.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_15.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_10.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_11.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_14.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_6.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_8.jpg
Saved sharpened image to: bottle/sharpened/test/good_9.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_4.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_16.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_9.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_13.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_12.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_8.jpg
Saved sharpened image to: bottle/sharpened/test/broken_small_17.jpg
Saved sharpened image to: bottle/sharpened/test/broken_large_5.jpg
Saved sharpened image to: bottle/sharpened/test/good_8.jpg
Saved sharpened image to: bottle/sharpened/test/contamination_9.jpg
Sharpened 83 images
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="training-model-for-sharpened-images-training-the-model-on-sharpened-images">
<h1>Training Model for Sharpened Images: Training the Model on Sharpened Images<a class="headerlink" href="#training-model-for-sharpened-images-training-the-model-on-sharpened-images" title="Link to this heading">#</a></h1>
<section id="id15">
<h2>Functionality Overview<a class="headerlink" href="#id15" title="Link to this heading">#</a></h2>
<p>This code block trains the <code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> on sharpened images using a similar workflow as the training on original images. It involves:</p>
<ul class="simple">
<li><p>Preparing a dataset and DataLoader for sharpened images.</p></li>
<li><p>Setting up a training loop with logging and loss tracking.</p></li>
<li><p>Saving the model and training metrics.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="key-differences">
<h2>Key Differences<a class="headerlink" href="#key-differences" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Dataset and DataLoader</strong>:</p>
<ul class="simple">
<li><p>Uses the sharpened images from <code class="docutils literal notranslate"><span class="pre">bottle/sharpened/train</span></code> with corresponding annotations in <code class="docutils literal notranslate"><span class="pre">bottle/sharpened/train_annotations.json</span></code>.</p></li>
<li><p>Applies data augmentation and normalization using a predefined transformation pipeline.</p></li>
</ul>
</li>
<li><p><strong>Model Saving</strong>:</p>
<ul class="simple">
<li><p>Saves the trained model weights to <code class="docutils literal notranslate"><span class="pre">models/sharpened-model.pth</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Loss Tracker</strong>:</p>
<ul class="simple">
<li><p>Stores training loss in <code class="docutils literal notranslate"><span class="pre">loss_per_epoch_batch_sharpened.csv</span></code> for later analysis.</p></li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul class="simple">
<li><p>Plots the loss trend over epochs and saves it as <code class="docutils literal notranslate"><span class="pre">Sharpened-loss.png</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="workflow-summary">
<h2>Workflow Summary<a class="headerlink" href="#workflow-summary" title="Link to this heading">#</a></h2>
<section id="dataset-preparation">
<h3>1. Dataset Preparation<a class="headerlink" href="#dataset-preparation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Creates a <code class="docutils literal notranslate"><span class="pre">CustomBottleDataset</span></code> for sharpened training images.</p></li>
<li><p>Wraps the dataset in a DataLoader (<code class="docutils literal notranslate"><span class="pre">sharpened_train_dataloader</span></code>) with a batch size of 5.</p></li>
</ul>
</section>
<section id="model-setup">
<h3>2. Model Setup<a class="headerlink" href="#model-setup" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Loads <code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> and pretrained weights if available (<code class="docutils literal notranslate"><span class="pre">sharpened-model.pth</span></code>).</p></li>
<li><p>Moves the model to the appropriate device (<code class="docutils literal notranslate"><span class="pre">mps</span></code> or <code class="docutils literal notranslate"><span class="pre">cpu</span></code>).</p></li>
</ul>
</section>
<section id="id16">
<h3>3. Training Loop<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Trains the model for 10 epochs, calculating and logging the loss every 10 batches.</p></li>
<li><p>Saves the model state after each batch to ensure progress is not lost.</p></li>
</ul>
</section>
<section id="loss-tracking">
<h3>4. Loss Tracking<a class="headerlink" href="#loss-tracking" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Appends average loss per epoch to the loss tracker file (<code class="docutils literal notranslate"><span class="pre">loss_per_epoch_batch_sharpened.csv</span></code>).</p></li>
</ul>
</section>
<section id="visualization">
<h3>5. Visualization<a class="headerlink" href="#visualization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Plots the loss trend for the training process, showing the modelâ€™s performance improvement.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="outputs">
<h2>Outputs<a class="headerlink" href="#outputs" title="Link to this heading">#</a></h2>
<section id="model-weights">
<h3>1. Model Weights:<a class="headerlink" href="#model-weights" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Saved to <code class="docutils literal notranslate"><span class="pre">models/sharpened-model.pth</span></code>.</p></li>
</ul>
</section>
<section id="loss-tracker-file">
<h3>2. Loss Tracker File:<a class="headerlink" href="#loss-tracker-file" title="Link to this heading">#</a></h3>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_per_epoch_batch_sharpened.csv</span></code>:</p>
<div class="highlight-csv notranslate"><div class="highlight"><pre><span></span>Loss
0.512
0.395
0.287
</pre></div>
</div>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## train the model on the sharpened images</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">),</span> <span class="n">antialias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="n">degrees</span><span class="o">=</span><span class="mi">45</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
<span class="p">])</span>

<span class="n">sharpened_train_dataset</span> <span class="o">=</span> <span class="n">CustomBottleDataset</span><span class="p">(</span><span class="n">data_directory</span><span class="o">=</span><span class="s1">&#39;bottle/sharpened/train&#39;</span><span class="p">,</span> <span class="n">labels_path</span><span class="o">=</span><span class="s1">&#39;bottle/sharpened/train_annotations.json&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
<span class="n">sharpened_train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">sharpened_train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">DefectIdentificationNetwork</span><span class="p">()</span>

<span class="n">sharpened_model_path</span> <span class="o">=</span> <span class="s1">&#39;models/sharpened-model.pth&#39;</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">sharpened_model_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Using saved model.&#39;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">sharpened_model_path</span><span class="p">))</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Type of device used for training: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># Put network into training mode, where neural network weights can change</span>

<span class="n">loss_arr</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sharpened_train_dataloader</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Print every 10 batches</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">], Step [</span><span class="si">{</span><span class="n">batch_idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">sharpened_model_path</span><span class="p">)</span>


    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
    <span class="n">loss_arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Average Loss after epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">loss_tracker_base_path</span> <span class="o">=</span> <span class="s2">&quot;reports/loss_tracker/loss_per_epoch_batch_sharpened.csv&quot;</span>

<span class="n">sharpened_loss_tracker</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">loss_tracker_base_path</span><span class="p">):</span>
    <span class="n">sharpened_loss_tracker</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">loss_tracker_base_path</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">sharpened_loss_tracker</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Loss&quot;</span> <span class="p">:</span> <span class="p">[]})</span>

<span class="c1">## loss tracker example format - &quot;Loss&quot; : [0.5, 0.3, 0.2]</span>
<span class="n">average_loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss_arr</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_epochs</span>
<span class="n">sharpened_loss_tracker</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">sharpened_loss_tracker</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Loss&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">average_loss</span><span class="p">]})],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">sharpened_loss_tracker</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">loss_tracker_base_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_arr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss Over Time - Sharpened Images&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;Sharpened-loss.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using saved model.
Type of device used for training: mps
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/v4/d7x3sgjd0zs0jbb4ytx6w2gh0000gn/T/ipykernel_98743/2562476472.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don&#39;t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(sharpened_model_path))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch [1/10], Step [10/54], Loss: 0.003471443895250559
Epoch [1/10], Step [20/54], Loss: 0.05210641026496887
Epoch [1/10], Step [30/54], Loss: 0.01604538783431053
Epoch [1/10], Step [40/54], Loss: 0.029340337961912155
Epoch [1/10], Step [50/54], Loss: 0.5955489873886108
Average Loss after epoch 1: 0.21731504412255837
Epoch [2/10], Step [10/54], Loss: 0.003987263888120651
Epoch [2/10], Step [20/54], Loss: 0.10268616676330566
Epoch [2/10], Step [30/54], Loss: 0.013473421335220337
Epoch [2/10], Step [40/54], Loss: 0.01574789546430111
Epoch [2/10], Step [50/54], Loss: 0.007403192110359669
Average Loss after epoch 2: 0.032714367723437374
Epoch [3/10], Step [10/54], Loss: 0.06604593992233276
Epoch [3/10], Step [20/54], Loss: 0.010262705385684967
Epoch [3/10], Step [30/54], Loss: 0.013405436649918556
Epoch [3/10], Step [40/54], Loss: 0.0043309880420565605
Epoch [3/10], Step [50/54], Loss: 0.051868993788957596
Average Loss after epoch 3: 0.1969598343604486
Epoch [4/10], Step [10/54], Loss: 0.02417992614209652
Epoch [4/10], Step [20/54], Loss: 0.15243878960609436
Epoch [4/10], Step [30/54], Loss: 0.6290184855461121
Epoch [4/10], Step [40/54], Loss: 0.10177342593669891
Epoch [4/10], Step [50/54], Loss: 0.32628777623176575
Average Loss after epoch 4: 0.11016688736897239
Epoch [5/10], Step [10/54], Loss: 0.31555047631263733
Epoch [5/10], Step [20/54], Loss: 0.7623885869979858
Epoch [5/10], Step [30/54], Loss: 0.05065014958381653
Epoch [5/10], Step [40/54], Loss: 0.052066367119550705
Epoch [5/10], Step [50/54], Loss: 0.024955328553915024
Average Loss after epoch 5: 0.07962099046239422
Epoch [6/10], Step [10/54], Loss: 0.00535154202952981
Epoch [6/10], Step [20/54], Loss: 0.047236181795597076
Epoch [6/10], Step [30/54], Loss: 0.40952032804489136
Epoch [6/10], Step [40/54], Loss: 0.42778095602989197
Epoch [6/10], Step [50/54], Loss: 0.03267399221658707
Average Loss after epoch 6: 0.0989217809315219
Epoch [7/10], Step [10/54], Loss: 0.1831902712583542
Epoch [7/10], Step [20/54], Loss: 0.016071287915110588
Epoch [7/10], Step [30/54], Loss: 0.007881846278905869
Epoch [7/10], Step [40/54], Loss: 0.04878806322813034
Epoch [7/10], Step [50/54], Loss: 0.01509067602455616
Average Loss after epoch 7: 0.08009134981289713
Epoch [8/10], Step [10/54], Loss: 0.03504995256662369
Epoch [8/10], Step [20/54], Loss: 0.011782866902649403
Epoch [8/10], Step [30/54], Loss: 0.020392876118421555
Epoch [8/10], Step [40/54], Loss: 0.1468007117509842
Epoch [8/10], Step [50/54], Loss: 0.006584255490452051
Average Loss after epoch 8: 0.06350040802068112
Epoch [9/10], Step [10/54], Loss: 0.0013146752025932074
Epoch [9/10], Step [20/54], Loss: 0.002501219045370817
Epoch [9/10], Step [30/54], Loss: 0.008287779986858368
Epoch [9/10], Step [40/54], Loss: 0.002341874409466982
Epoch [9/10], Step [50/54], Loss: 0.05379172042012215
Average Loss after epoch 9: 0.04117865227507772
Epoch [10/10], Step [10/54], Loss: 0.33449047803878784
Epoch [10/10], Step [20/54], Loss: 0.0028190421871840954
Epoch [10/10], Step [30/54], Loss: 0.022603880614042282
Epoch [10/10], Step [40/54], Loss: 0.045856770128011703
Epoch [10/10], Step [50/54], Loss: 0.005803986452519894
Average Loss after epoch 10: 0.15748466025710245
</pre></div>
</div>
<img alt="_images/b077d51e61e5bb546c9ba8ed4fba8a6083df72eb4411c54a6ee0c5d9252c997d.png" src="_images/b077d51e61e5bb546c9ba8ed4fba8a6083df72eb4411c54a6ee0c5d9252c997d.png" />
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="validation-step-for-model-using-sharpened-images-evaluation-on-sharpened-test-images">
<h1>Validation Step for Model Using Sharpened Images: Evaluation on Sharpened Test Images<a class="headerlink" href="#validation-step-for-model-using-sharpened-images-evaluation-on-sharpened-test-images" title="Link to this heading">#</a></h1>
<section id="id17">
<h2>Functionality Overview<a class="headerlink" href="#id17" title="Link to this heading">#</a></h2>
<p>This code block evaluates the <code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> on sharpened test images using a similar workflow as the evaluation on original test images. It involves:</p>
<ul class="simple">
<li><p>Loading the sharpened test dataset.</p></li>
<li><p>Making predictions and logging results.</p></li>
<li><p>Calculating accuracy and misclassification rate.</p></li>
<li><p>Saving results and metrics for further analysis.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id18">
<h2>Key Differences<a class="headerlink" href="#id18" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Dataset and DataLoader</strong>:</p>
<ul class="simple">
<li><p>Evaluates on sharpened images from <code class="docutils literal notranslate"><span class="pre">bottle/sharpened/test</span></code> with annotations in <code class="docutils literal notranslate"><span class="pre">bottle/sharpened/test_annotations.json</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Model Weights</strong>:</p>
<ul class="simple">
<li><p>Loads weights from <code class="docutils literal notranslate"><span class="pre">sharpened-model.pth</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Confusion Matrix File</strong>:</p>
<ul class="simple">
<li><p>Saves classification results to <code class="docutils literal notranslate"><span class="pre">reports/sharpened_conf_matrix.csv</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul class="simple">
<li><p>Displays misclassified sharpened images with true and predicted labels.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="id19">
<h2>Outputs<a class="headerlink" href="#id19" title="Link to this heading">#</a></h2>
<section id="accuracy-and-misclassification-rate">
<h3>1. Accuracy and Misclassification Rate:<a class="headerlink" href="#accuracy-and-misclassification-rate" title="Link to this heading">#</a></h3>
<ul>
<li><p>Prints overall accuracy and the percentage of misclassified images:</p>
<div class="highlight-plaintext notranslate"><div class="highlight"><pre><span></span>Accuracy: 93.5%
Number of false classifications: 10
Misclassification rate: % 6.5
</pre></div>
</div>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">),</span> <span class="n">antialias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="n">degrees</span><span class="o">=</span><span class="mi">15</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
      <span class="n">v2</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
<span class="p">])</span>

<span class="n">sharpened_test_dataset</span> <span class="o">=</span> <span class="n">CustomBottleDataset</span><span class="p">(</span><span class="n">data_directory</span><span class="o">=</span><span class="s1">&#39;bottle/sharpened/test&#39;</span><span class="p">,</span> <span class="n">labels_path</span><span class="o">=</span><span class="s1">&#39;bottle/sharpened/test_annotations.json&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
<span class="n">sharpened_test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">sharpened_test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">groundtruth</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DefectIdentificationNetwork</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">sharpened_model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Ensure the model is in evaluation mode</span>


<span class="c1"># Reverse the label mapping for easier interpretation</span>
<span class="n">label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;good&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;broken-small&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;broken-large&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;contamination&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">reverse_label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">label_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">filenames</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sharpened_test_dataset</span><span class="o">.</span><span class="n">image_labels</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="n">global_idx</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">conf_matrix_mapping</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Image&quot;</span> <span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;Actual&quot;</span> <span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;Predicted&quot;</span> <span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;Result&quot;</span> <span class="p">:</span> <span class="p">[]</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sharpened_test_loader</span><span class="p">):</span>
    <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">)</span>

    <span class="c1"># Perform forward pass without disabling gradients</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">groundtruth</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)):</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">true_label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">pred_label</span> <span class="o">=</span> <span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Write classification results to the file</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;classification_results.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">log_file</span><span class="p">:</span>
            <span class="n">log_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Image: </span><span class="si">{</span><span class="n">filenames</span><span class="p">[</span><span class="n">global_idx</span><span class="p">]</span><span class="si">}</span><span class="s2">, True: </span><span class="si">{</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">true_label</span><span class="p">]</span><span class="si">}</span><span class="s2">, Pred: </span><span class="si">{</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">pred_label</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Reverse normalization for visualization</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">*</span> <span class="n">std</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">mean</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>

        <span class="c1"># Plot if prediction is incorrect</span>
        <span class="k">if</span> <span class="n">pred_label</span> <span class="o">!=</span> <span class="n">true_label</span><span class="p">:</span>

            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Filename: </span><span class="si">{</span><span class="n">filenames</span><span class="p">[</span><span class="n">global_idx</span><span class="p">]</span><span class="si">}</span><span class="s2"> True: </span><span class="si">{</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">true_label</span><span class="p">]</span><span class="si">}</span><span class="s2">, Pred: </span><span class="si">{</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">pred_label</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
       
        <span class="n">conf_matrix_mapping</span><span class="p">[</span><span class="s1">&#39;Image&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filenames</span><span class="p">[</span><span class="n">global_idx</span><span class="p">])</span>
        <span class="n">conf_matrix_mapping</span><span class="p">[</span><span class="s1">&#39;Actual&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">true_label</span><span class="p">])</span>
        <span class="n">conf_matrix_mapping</span><span class="p">[</span><span class="s1">&#39;Predicted&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reverse_label_mapping</span><span class="p">[</span><span class="n">pred_label</span><span class="p">])</span>
        <span class="n">conf_matrix_mapping</span><span class="p">[</span><span class="s1">&#39;Result&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_label</span> <span class="o">==</span> <span class="n">true_label</span><span class="p">)</span>

        <span class="n">global_idx</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Increment global index</span>

<span class="c1"># Calculate accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">groundtruth</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>

<span class="n">conf_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">conf_matrix_mapping</span><span class="p">)</span>

<span class="n">report_path</span> <span class="o">=</span> <span class="s2">&quot;reports/sharpened_conf_matrix.csv&quot;</span>

<span class="n">conf_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">report_path</span><span class="p">)</span>

<span class="c1"># Count the number of False results</span>
<span class="n">num_false</span> <span class="o">=</span> <span class="p">(</span><span class="n">conf_df</span><span class="p">[</span><span class="s1">&#39;Result&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># Total number of rows</span>
<span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">conf_df</span><span class="p">)</span>

<span class="c1"># Calculate misclassification rate</span>
<span class="n">misclassification_rate</span> <span class="o">=</span> <span class="n">num_false</span> <span class="o">/</span> <span class="n">total</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of false classifications:&quot;</span><span class="p">,</span> <span class="n">num_false</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Misclassification rate: %&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">misclassification_rate</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/v4/d7x3sgjd0zs0jbb4ytx6w2gh0000gn/T/ipykernel_98743/127485768.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don&#39;t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(sharpened_model_path, map_location=device))
</pre></div>
</div>
<img alt="_images/6af47bc95414af291cef817cc7eb7a10c0dc0d3c1b9b3dd622973ce2ac822733.png" src="_images/6af47bc95414af291cef817cc7eb7a10c0dc0d3c1b9b3dd622973ce2ac822733.png" />
<img alt="_images/b1d32f000e524805ccf4f8d9b304a149c254f21323ed367dca242cb680896273.png" src="_images/b1d32f000e524805ccf4f8d9b304a149c254f21323ed367dca242cb680896273.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 97.59036144578313%
Number of false classifications: 2
Misclassification rate: % 2.4096385542168677
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="analyzing-the-base-model-visualizing-loss-over-epochs">
<h1>Analyzing the Base Model: Visualizing Loss Over Epochs<a class="headerlink" href="#analyzing-the-base-model-visualizing-loss-over-epochs" title="Link to this heading">#</a></h1>
<section id="id20">
<h2>Functionality Overview<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<p>This code block reads loss data from a CSV file, processes it, and visualizes the loss values over epochs. The loss is plotted as a line graph to provide insights into the base modelâ€™s training performance.</p>
</section>
<hr class="docutils" />
<section id="key-steps">
<h2>Key Steps<a class="headerlink" href="#key-steps" title="Link to this heading">#</a></h2>
<section id="check-for-csv-file">
<h3><strong>1. Check for CSV File</strong><a class="headerlink" href="#check-for-csv-file" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>File Path</strong>:</p>
<ul class="simple">
<li><p>The loss data is read from <code class="docutils literal notranslate"><span class="pre">reports/loss_tracker/loss_per_epoch_batch_base.csv</span></code>.</p></li>
</ul>
</li>
<li><p><strong>File Existence Check</strong>:</p>
<ul>
<li><p>If the file does not exist, a message is printed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">base_loss_data_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Required CSV file does not exist yet.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="read-loss-data">
<h3><strong>2. Read Loss Data</strong><a class="headerlink" href="#read-loss-data" title="Link to this heading">#</a></h3>
<ul>
<li><p>The script reads the CSV file using <code class="docutils literal notranslate"><span class="pre">pandas</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">base_loss_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">base_loss_data_path</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">base_loss_data_path</span> <span class="o">=</span> <span class="s2">&quot;reports/loss_tracker/loss_per_epoch_batch_base.csv&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">base_loss_data_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Required CSV file does not exist yet.&#39;</span><span class="p">)</span>

<span class="n">base_loss_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">base_loss_data_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">base_loss_data</span><span class="p">))</span>
<span class="n">base_loss_data</span><span class="p">[</span><span class="s2">&quot;Index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_loss_data</span><span class="o">.</span><span class="n">index</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">base_loss_data</span><span class="p">[</span><span class="s2">&quot;Loss&quot;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss Over Epochs (Grouped in Increments of 10)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch Group (10 Epochs per Group)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Average Loss&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2
</pre></div>
</div>
<img alt="_images/aa3487a75fcd5974ae36854eab6c310d4fd591ea76c53f67ad52481940410a10.png" src="_images/aa3487a75fcd5974ae36854eab6c310d4fd591ea76c53f67ad52481940410a10.png" />
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="code-summary-visualizing-loss-over-epochs-for-sharpened-images">
<h1>Code Summary: Visualizing Loss Over Epochs for Sharpened Images<a class="headerlink" href="#code-summary-visualizing-loss-over-epochs-for-sharpened-images" title="Link to this heading">#</a></h1>
<section id="id21">
<h2>Functionality Overview<a class="headerlink" href="#id21" title="Link to this heading">#</a></h2>
<p>This code block visualizes the loss values over epochs for the model trained on sharpened images. It reads the loss data from a CSV file and plots it as a line graph.</p>
</section>
<hr class="docutils" />
<section id="id22">
<h2>Key Steps<a class="headerlink" href="#id22" title="Link to this heading">#</a></h2>
<section id="id23">
<h3><strong>1. Check for CSV File</strong><a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>File Path</strong>:</p>
<ul>
<li><p>Loss data is read from <code class="docutils literal notranslate"><span class="pre">reports/loss_tracker/loss_per_epoch_batch_sharpened.csv</span></code>.</p></li>
</ul>
</li>
<li><p><strong>File Existence Check</strong>:</p>
<ul>
<li><p>Ensures the file exists before proceeding.</p></li>
</ul>
</li>
</ul>
</section>
<section id="read-and-process-data">
<h3><strong>2. Read and Process Data</strong><a class="headerlink" href="#read-and-process-data" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Reads the CSV file into a pandas DataFrame.</p></li>
<li><p>Plots the <code class="docutils literal notranslate"><span class="pre">Loss</span></code> column, representing the loss for each epoch.</p></li>
</ul>
</section>
<section id="plot-the-loss">
<h3><strong>3. Plot the Loss</strong><a class="headerlink" href="#plot-the-loss" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Graph Details</strong>:</p>
<ul>
<li><p>X-Axis: Epochs.</p></li>
<li><p>Y-Axis: Average Loss.</p></li>
<li><p>Title: â€œLoss Over Epochs (Grouped in Increments of 10) - Sharpened Imagesâ€.</p></li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul>
<li><p>Displays a line graph with markers and a grid for clarity.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sharpened_loss_data_path</span> <span class="o">=</span> <span class="s2">&quot;reports/loss_tracker/loss_per_epoch_batch_sharpened.csv&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">sharpened_loss_data_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Required CSV file does not exist yet.&#39;</span><span class="p">)</span>

<span class="n">sharpened_loss_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">sharpened_loss_data_path</span><span class="p">)</span>

<span class="n">sharpened_loss_data</span><span class="p">[</span><span class="s2">&quot;Index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sharpened_loss_data</span><span class="o">.</span><span class="n">index</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sharpened_loss_data</span><span class="p">[</span><span class="s2">&quot;Loss&quot;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss Over Epochs (Grouped in Increments of 10) - Sharpened Images&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch Group (10 Epochs per Group)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Average Loss&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ed2835b26f9395cbb0969b1972c685c041891273fe8a09795069f1b6e585d9bc.png" src="_images/ed2835b26f9395cbb0969b1972c685c041891273fe8a09795069f1b6e585d9bc.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>

<span class="n">conf_base_path</span> <span class="o">=</span> <span class="s2">&quot;reports/base_conf_matrix.csv&quot;</span>
<span class="n">conf_sharpened_path</span> <span class="o">=</span> <span class="s2">&quot;reports/sharpened_conf_matrix.csv&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">conf_base_path</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">conf_sharpened_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Required CSV files have not been generated&#39;</span><span class="p">)</span>

<span class="n">conf_base_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">conf_base_path</span><span class="p">)</span>
<span class="n">conf_sharpened_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">conf_sharpened_path</span><span class="p">)</span>

<span class="n">actual_base</span> <span class="o">=</span> <span class="n">conf_base_df</span><span class="p">[</span><span class="s2">&quot;Actual&quot;</span><span class="p">]</span>
<span class="n">predicted_base</span> <span class="o">=</span> <span class="n">conf_base_df</span><span class="p">[</span><span class="s2">&quot;Predicted&quot;</span><span class="p">]</span>

<span class="n">precision_base</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">actual_base</span><span class="p">,</span> <span class="n">predicted_base</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;weighted&quot;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">recall_base</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">actual_base</span><span class="p">,</span> <span class="n">predicted_base</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;weighted&quot;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">f1_base</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">actual_base</span><span class="p">,</span> <span class="n">predicted_base</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;weighted&quot;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">base_model_metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Metric&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Precision&quot;</span><span class="p">,</span> <span class="s2">&quot;Recall&quot;</span><span class="p">,</span> <span class="s2">&quot;F1 Score&quot;</span><span class="p">],</span>
    <span class="s2">&quot;Value&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">precision_base</span><span class="p">,</span> <span class="n">recall_base</span><span class="p">,</span> <span class="n">f1_base</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">base_model_metrics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Precision</td>
      <td>0.830079</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Recall</td>
      <td>0.771084</td>
    </tr>
    <tr>
      <th>2</th>
      <td>F1 Score</td>
      <td>0.760929</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">conf_sharpened_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">conf_sharpened_path</span><span class="p">)</span>

<span class="n">actual_sharp</span> <span class="o">=</span> <span class="n">conf_sharpened_df</span><span class="p">[</span><span class="s2">&quot;Actual&quot;</span><span class="p">]</span>
<span class="n">predicted_sharp</span> <span class="o">=</span> <span class="n">conf_sharpened_df</span><span class="p">[</span><span class="s2">&quot;Predicted&quot;</span><span class="p">]</span>

<span class="n">precision_sharp</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">actual_sharp</span><span class="p">,</span> <span class="n">predicted_sharp</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;weighted&quot;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">recall_sharp</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">actual_sharp</span><span class="p">,</span> <span class="n">predicted_sharp</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;weighted&quot;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">f1_sharp</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">actual_sharp</span><span class="p">,</span> <span class="n">predicted_sharp</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;weighted&quot;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">sharp_model_metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Metric&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Precision&quot;</span><span class="p">,</span> <span class="s2">&quot;Recall&quot;</span><span class="p">,</span> <span class="s2">&quot;F1 Score&quot;</span><span class="p">],</span>
    <span class="s2">&quot;Value&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">precision_sharp</span><span class="p">,</span> <span class="n">recall_sharp</span><span class="p">,</span> <span class="n">f1_sharp</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">sharp_model_metrics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Precision</td>
      <td>0.977001</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Recall</td>
      <td>0.975904</td>
    </tr>
    <tr>
      <th>2</th>
      <td>F1 Score</td>
      <td>0.975582</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">comparison_metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Metric&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Precision&quot;</span><span class="p">,</span> <span class="s2">&quot;Recall&quot;</span><span class="p">,</span> <span class="s2">&quot;F1 Score&quot;</span><span class="p">],</span>
    <span class="s2">&quot;Base Model&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">precision_base</span><span class="p">,</span> <span class="n">recall_base</span><span class="p">,</span> <span class="n">f1_base</span><span class="p">],</span>
    <span class="s2">&quot;Sharpened Model&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">precision_sharp</span><span class="p">,</span> <span class="n">recall_sharp</span><span class="p">,</span> <span class="n">f1_sharp</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">comparison_metrics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Metric</th>
      <th>Base Model</th>
      <th>Sharpened Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Precision</td>
      <td>0.830079</td>
      <td>0.977001</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Recall</td>
      <td>0.771084</td>
      <td>0.975904</td>
    </tr>
    <tr>
      <th>2</th>
      <td>F1 Score</td>
      <td>0.760929</td>
      <td>0.975582</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Welcome to your Jupyter Book</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Preprocessing Data -  Renaming Training Image Files with Unique Numbering</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#functionality-overview">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-details">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#count-files-directory-function"><strong><code class="docutils literal notranslate"><span class="pre">count_files(directory)</span></code> Function</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-image-count"><strong>Training Image Count</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#file-renaming-logic"><strong>File Renaming Logic</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-step-for-pytorch-cnn-creating-json-annotations-for-dataloader-pytorch">Preprocessing Step for Pytorch CNN: Creating JSON Annotations for DataLoader (Pytorch)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-json-labels-directory-output-file-function"><strong><code class="docutils literal notranslate"><span class="pre">create_json_labels(directory,</span> <span class="pre">output_file)</span></code> Function</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-preparation-custom-dataset-and-dataloader-for-bottle-classification">PyTorch Preparation: Custom Dataset and DataLoader for Bottle Classification</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custombottledataset-class"><strong><code class="docutils literal notranslate"><span class="pre">CustomBottleDataset</span></code> Class</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initialization-init"><strong>Initialization (<code class="docutils literal notranslate"><span class="pre">__init__</span></code>)</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#length-len"><strong>Length (<code class="docutils literal notranslate"><span class="pre">__len__</span></code>)</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#item-retrieval-getitem"><strong>Item Retrieval (<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>)</strong>:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformations-t"><strong>Transformations (<code class="docutils literal notranslate"><span class="pre">T</span></code>)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-dataloader"><strong>Dataset and DataLoader</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-cnn-using-reference-model-defect-identification-neural-network">Custom CNN Using Reference Model: Defect Identification Neural Network</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defectidentificationnetwork-class"><strong><code class="docutils literal notranslate"><span class="pre">DefectIdentificationNetwork</span></code> Class</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7"><strong>Initialization (<code class="docutils literal notranslate"><span class="pre">__init__</span></code>)</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-forward"><strong>Forward Pass (<code class="docutils literal notranslate"><span class="pre">forward</span></code>)</strong>:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#training-step-training-and-tracking-the-defect-identification-model">Training Step: Training and Tracking the Defect Identification Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-initialization-and-loading"><strong>1. Model Initialization and Loading</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-configuration"><strong>2. Training Configuration</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop"><strong>3. Training Loop</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-tracking-and-reporting"><strong>4. Loss Tracking and Reporting</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-step-model-evaluation-and-performance-reporting">Validation Step: Model Evaluation and Performance Reporting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-dataloader-setup"><strong>1. Dataset and DataLoader Setup</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-initialization"><strong>2. Model Initialization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-loop"><strong>3. Evaluation Loop</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-calculation"><strong>4. Metrics Calculation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-files"><strong>5. Output Files</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-for-model-using-sharpened-images-sharpening-images-and-generating-annotations">Preprocessing for Model Using Sharpened Images: Sharpening Images and Generating Annotations</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Code Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sharpening-images"><strong>1. Sharpening Images</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-parameters"><strong>Input Parameters</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14"><strong>Steps</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output"><strong>Output</strong>:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-sharpened-image-annotations"><strong>2. Generating Sharpened Image Annotations</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#directory-structure"><strong>3. Directory Structure</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#training-model-for-sharpened-images-training-the-model-on-sharpened-images">Training Model for Sharpened Images: Training the Model on Sharpened Images</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences">Key Differences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-summary">Workflow Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-preparation">1. Dataset Preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-setup">2. Model Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3. Training Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-tracking">4. Loss Tracking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization">5. Visualization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outputs">Outputs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-weights">1. Model Weights:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-tracker-file">2. Loss Tracker File:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-step-for-model-using-sharpened-images-evaluation-on-sharpened-test-images">Validation Step for Model Using Sharpened Images: Evaluation on Sharpened Test Images</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Key Differences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Outputs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-and-misclassification-rate">1. Accuracy and Misclassification Rate:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-the-base-model-visualizing-loss-over-epochs">Analyzing the Base Model: Visualizing Loss Over Epochs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-steps">Key Steps</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-for-csv-file"><strong>1. Check for CSV File</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#read-loss-data"><strong>2. Read Loss Data</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#code-summary-visualizing-loss-over-epochs-for-sharpened-images">Code Summary: Visualizing Loss Over Epochs for Sharpened Images</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Functionality Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Key Steps</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23"><strong>1. Check for CSV File</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#read-and-process-data"><strong>2. Read and Process Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss"><strong>3. Plot the Loss</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>